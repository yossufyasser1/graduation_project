{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.24.3)\n",
      "Collecting argparse\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (9.4.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.20.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.65.0)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.29.0)\n",
      "Requirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.29.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: argparse, pyparsing, kiwisolver, h5py, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed argparse-1.4.0 contourpy-1.0.7 cycler-0.11.0 fonttools-4.39.4 h5py-3.8.0 kiwisolver-1.4.4 matplotlib-3.7.1 pyparsing-3.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch matplotlib numpy argparse Pillow torchvision scikit-image tqdm h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G72ZhH8jxQWZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import argparse\n",
    "import pickle \n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision.transforms as transformsZ\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "import argparse\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from random import seed, choice, sample\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QOsPk4LR2gZB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the URLs for downloading the dataset\n",
    "dataset_url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n",
    "annotations_url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
    "\n",
    "# Set the paths for saving the dataset\n",
    "dataset_save_path = \"/Flickr8k_Dataset.zip\"\n",
    "annotations_save_path = \"/Flickr8k.zip\"\n",
    "\n",
    "# Download the dataset and annotations\n",
    "urllib.request.urlretrieve(dataset_url, dataset_save_path)\n",
    "urllib.request.urlretrieve(annotations_url, annotations_save_path)\n",
    "\n",
    "# Unzip the dataset\n",
    "with zipfile.ZipFile(dataset_save_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"/Flickr8k_Dataset\")\n",
    "\n",
    "# Unzip the annotations\n",
    "with zipfile.ZipFile(annotations_save_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"/Flickr8k_text\")\n",
    "\n",
    "# Remove the zip files\n",
    "os.remove(dataset_save_path)\n",
    "os.remove(annotations_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dwayne99/Image_Captioning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependancies\n",
    "import json\n",
    "from collections import Counter\n",
    "from random import seed, choice, sample\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from cv2 import imread, resize\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_input_files(dataset, json_path, image_folder, captions_per_image,\n",
    "                       min_word_freq, output_folder, max_len = 100):\n",
    "\n",
    "    '''\n",
    "    Creates input files for training, validation, and test data.\n",
    "    :param dataset: name of dataset, one of 'coco', 'flickr8k', 'flickr30k'\n",
    "    :param json_path: path of Karpathy JSON file with splits and captions\n",
    "    :param image_folder: folder with downloaded images\n",
    "    :param captions_per_image: number of captions to sample per image\n",
    "    :param min_word_freq: words occuring less frequently than this threshold are binned as <unk>s\n",
    "    :param output_folder: folder to save files\n",
    "    :param max_len: don't sample captions longer than this length\n",
    "    '''\n",
    "\n",
    "    assert dataset in {'coco', 'flickr8k', 'flickr30k'}\n",
    "\n",
    "    # Read Karpathy JSON\n",
    "    with open(json_path, 'r') as j:\n",
    "        data = json.load(j)\n",
    "\n",
    "    # Read image paths and captions for each image\n",
    "    train_image_paths = []\n",
    "    train_image_captions = []\n",
    "    val_image_paths = []\n",
    "    val_image_captions = []\n",
    "    test_image_paths = []\n",
    "    test_image_captions = []\n",
    "    word_freq = Counter()\n",
    "\n",
    "    # iterate over data\n",
    "    for img in data['images']:\n",
    "        captions = []\n",
    "\n",
    "        # iterate over each caption of an image\n",
    "        for c in img['sentences']:\n",
    "\n",
    "            # update word frequency\n",
    "            word_freq.update(c['tokens'])\n",
    "            \n",
    "            # don't save caption that exceeds max_len\n",
    "            if len(c['tokens']) <= max_len:\n",
    "                captions.append(c['tokens'])\n",
    "\n",
    "        # if all captions of an image don't meet max_len criteria don't save the image\n",
    "        if len(captions) == 0:\n",
    "            continue\n",
    "\n",
    "        # construct the image path\n",
    "        path = os.path.join(image_folder, img['filename'])\n",
    "\n",
    "        # Check the value of the split attribute to place image in the desired folder\n",
    "        if img['split'] in {'train', 'restval'}:\n",
    "            train_image_paths.append(path)\n",
    "            train_image_captions.append(captions)\n",
    "        elif img['split'] in {'val'}:\n",
    "            val_image_paths.append(path)\n",
    "            val_image_captions.append(captions)\n",
    "        elif img['split'] in {'test'}:\n",
    "            test_image_paths.append(path)\n",
    "            test_image_captions.append(captions)\n",
    "            \n",
    "\n",
    "    # Sanity check\n",
    "    assert len(train_image_paths) == len(train_image_captions)\n",
    "    assert len(val_image_paths) == len(val_image_captions)\n",
    "    assert len(test_image_paths) == len(test_image_captions)\n",
    "\n",
    "    # Create the word map\n",
    "\n",
    "    # shortlist the words that meet min_word_freq criteria\n",
    "    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "    word_map = {k: v for v, k in enumerate(words,1)}\n",
    "    word_map['<unk>'] = len(word_map) + 1\n",
    "    word_map['<start>'] = len(word_map) + 1\n",
    "    word_map['<end>'] = len(word_map) + 1\n",
    "    word_map['<pad>'] = 0\n",
    "\n",
    "    # Create a base/root name for all output files\n",
    "    base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
    "\n",
    "    # Save word map to a JSON\n",
    "    with open(os.path.join(output_folder, 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n",
    "        json.dump(word_map, j)\n",
    "\n",
    "    # Sample captions for each image, save images to HDF5 file, and captions along with their lengths to JSON files\n",
    "    seed(123)\n",
    "\n",
    "    for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
    "                                   (val_image_paths, val_image_captions, 'VAL'),\n",
    "                                   (test_image_paths, test_image_captions, 'TEST')]:\n",
    "\n",
    "        with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n",
    "            # Make a note of the num of captions we're sampling per image\n",
    "            h.attrs['captions_per_image'] = captions_per_image\n",
    "\n",
    "            # Create a dataset inside HDF5 file to store images\n",
    "            images = h.create_dataset('Images', (len(impaths), 3, 256, 256), dtype='uint8')\n",
    "\n",
    "            print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n",
    "\n",
    "            enc_captions = []\n",
    "            caplens = []\n",
    "\n",
    "            for i, path in enumerate(tqdm(impaths)):\n",
    "\n",
    "                # Sample captions \n",
    "                if len(imcaps[i]) < captions_per_image:\n",
    "                    captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n",
    "                else:\n",
    "                    captions = sample(imcaps[i], k=captions_per_image)\n",
    "\n",
    "                # Sanity check\n",
    "                assert len(captions) == captions_per_image\n",
    "\n",
    "                # Read images\n",
    "                img = imread(impaths[i])\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                # if image is grayscale add the depth dimention to it\n",
    "                if len(img.shape) == 2 :\n",
    "                    img = img[:,:, np.newaxis]\n",
    "                    img = np.concatenate([img, img, img], axis=2)\n",
    "                # Resize the image \n",
    "                img = resize(img, (256,256))\n",
    "                # convert image from H x W x C -->  C x H x W\n",
    "                img = img.transpose(2,0,1)\n",
    "\n",
    "                assert img.shape == (3, 256, 256)\n",
    "                assert np.max(img) <= 255\n",
    "\n",
    "                # Save image to HDF5 file\n",
    "                images[i] = img\n",
    "\n",
    "                for j, c in enumerate(captions):\n",
    "                    # Encode captions\n",
    "                    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [\n",
    "                        word_map['<end>']] + [word_map['<pad>']] * (max_len - len(c))\n",
    "\n",
    "                    # Find caption lengths, add 2 for 'start' and 'end' tokens\n",
    "                    c_len = len(c) + 2\n",
    "\n",
    "                    enc_captions.append(enc_c)\n",
    "                    caplens.append(c_len)\n",
    "\n",
    "            # Sanity check\n",
    "            assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n",
    "\n",
    "            # Save encoded captions and their lengths to JSON files\n",
    "            with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(enc_captions, j)\n",
    "\n",
    "            with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(caplens, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_input_files(\n",
    "        dataset='flickr8k',\n",
    "        json_path = 'Image_Captioning/data/dataset_flickr8k.json',\n",
    "        image_folder='Flickr8k_Dataset/Flicker8k_Dataset',\n",
    "        captions_per_image=5,\n",
    "        min_word_freq=5,\n",
    "        output_folder='/',\n",
    "        max_len=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''Encoder Model'''\n",
    "\n",
    "    def __init__(self, encoded_image_size = 14):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        # pretrained ImageNet ResNet-101\n",
    "        resnet = torchvision.models.resnet101(pretrained=True)\n",
    "\n",
    "        # Remove linear and pool layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        # Resize the image to a fixed size to allow input images of variable size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size,encoded_image_size))\n",
    "\n",
    "        self.fine_tune()\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        '''\n",
    "        Forward propagation\n",
    "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        :return: encoded images\n",
    "        '''\n",
    "\n",
    "        out = self.resnet(images) # (batch_size, 2048, image_size/32, image_size/32)\n",
    "        out = self.adaptive_pool(out) # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        out = out.permute(0,2,3,1) # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        '''\n",
    "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "        :param fine_tune: Allow?\n",
    "        '''\n",
    "\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    '''Attention Network'''\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        # linear layer to transform encoded image\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  \n",
    "        # linear layer to transform decoder's output\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        # linear layer to calculate values to be softmax-ed\n",
    "        self.full_att = nn.Linear(attention_dim,1) \n",
    "        self.relu = nn.ReLU()\n",
    "        # softmax layer to calculate weights\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        :return: attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden) # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att) # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "        \n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    '''Decoder'''\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        '''\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param vocab_size: size of the vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        '''\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim  #2048\n",
    "        self.attention_dim = attention_dim   #512\n",
    "        self.embed_dim = embed_dim  #512\n",
    "        self.decoder_dim = decoder_dim  # 512\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Create an Attention Network Instance\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) \n",
    "        # Dropout Layer \n",
    "        self.dropout = nn.Dropout(p = self.dropout)\n",
    "        # Decoding LSTM\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim,bias=True)\n",
    "        # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        # linear layer to find initial cell state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        # linear layer to create a sigmoid activated gate\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # linear layer to find scores over vocab\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "        # initialize some layers with the uniform distribution\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence\n",
    "        '''\n",
    "        self.embedding.weight.data.uniform_(-0.1,0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Loads embedding layer with pre-trained embeddings.\n",
    "        :param embeddings: pre-trained embeddings\n",
    "        \"\"\"\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "\n",
    "    def fine_tune_embeddigs(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :return: hidden state, cell state\n",
    "        \"\"\"\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        # mean_encoder_out = [batch_size, encoder_dim]\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)  \n",
    "        return h, c\n",
    "\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions ,caption_lengths):\n",
    "        '''\n",
    "        Forward Propagation\n",
    "\n",
    "        :param encoder_out: encoded images, a tensor of dimention (batch_size,enc_image_size, enc_image_size, encoder_dim)\n",
    "        :param encoded_captions: encoded captions,  a tensor of dimension (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        '''\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1) # 2048\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # encoder_out.shape = (batch_size, 14,14,2048)\n",
    "        # Flatten the image:  \n",
    "        encoder_out= encoder_out.view(batch_size, -1, encoder_dim)\n",
    "        # encoder_out.shape = (batch_size, 14*14,2048)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths:\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        # caption_lengths = sorted([batch_size]), sort_ind = [batch_size]\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        # Embedding\n",
    "        embeddings = self.embedding(encoded_captions) \n",
    "        # embedding.shape = [batch_size, max_caption_length, embed_dim]\n",
    "\n",
    "        # Initialize the LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out) # (batch_size, decoder_dim)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # so, decoding lengths are actual lengths-1\n",
    "        decode_lengths = (caption_lengths -1 ).tolist()\n",
    "        # decode_lengths = [batch_size]\n",
    "\n",
    "        # create tensors to hold word prediction scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "\n",
    "        # At each time-step, decode by \n",
    "        # attention-weighing the encoder's output based on the decoder's based on the decoders previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([ l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t])) # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "\n",
    "            preds = self.fc(self.dropout(h)) # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "        \n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "##########################################################FUNCTIONS FOR TRAIN.PY\n",
    "def adjust_learning_rate(optimizer, shrink_factor):\n",
    "    '''\n",
    "    Shrinks the learning rate by a specified factor.\n",
    "\n",
    "    :param optimizer: optimizer whose learning rate must be shrunk\n",
    "    :param shrink_factor: factor in interval (0,1) to multiply learning rate with\n",
    "    '''\n",
    "\n",
    "    print('\\nDECAYING learning rate.')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr']*shrink_factor\n",
    "    print('The new learning rate is %f\\n' %(optimizer.param_groups[0]['lr'],))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.resnet()\n",
    "\n",
    "    def resnet(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum_ = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum_ += val*n\n",
    "        self.count +=  n\n",
    "        self.avg = self.sum_ / self.count\n",
    "        \n",
    "\n",
    "def clip_gradient(optimizer, grad_clip):\n",
    "    '''\n",
    "    clips gradients computed during backpropagation to avoid explosion of gradients.\n",
    "\n",
    "    :param optimizer: optimizer with the gradients to be clipped\n",
    "    :param grad_clip: clip value\n",
    "    '''\n",
    "\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "\n",
    "def accuracy(scores, targets, k):\n",
    "    '''\n",
    "    computes the top-k accuracy, from predicted and true labels.\n",
    "\n",
    "    :param scores: scores from the model\n",
    "    :param targets: true labels\n",
    "    :param k: k in top-k accuracy\n",
    "    :return: top-k accuracy\n",
    "    '''\n",
    "\n",
    "    batch_size = targets.size(0)\n",
    "    _, ind = scores.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() * (100.0 / batch_size)\n",
    "\n",
    "    \n",
    "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                    bleu4, is_best):\n",
    "    \"\"\"\n",
    "    Saves model checkpoint.\n",
    "    :param data_name: base name of processed dataset\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n",
    "    :param decoder_optimizer: optimizer to update decoder's weights\n",
    "    :param bleu4: validation BLEU-4 score for this epoch\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {'epoch': epoch,\n",
    "             'epochs_since_improvement': epochs_since_improvement,\n",
    "             'bleu-4': bleu4,\n",
    "             'encoder': encoder,\n",
    "             'decoder': decoder,\n",
    "             'encoder_optimizer': encoder_optimizer,\n",
    "             'decoder_optimizer': decoder_optimizer}\n",
    "    filename = 'checkpoint_' + data_name + '.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, 'BEST_' + filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:58: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:58: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/tmp/ipykernel_3203/1137577698.py:58: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if self.split is 'TRAIN':\n"
     ]
    }
   ],
   "source": [
    "# Dependancies\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder, data_name, split, transform=None):\n",
    "        \"\"\"\n",
    "        :param data_folder: folder where data files are stored\n",
    "        :param data_name: base name of processed datasets\n",
    "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
    "        :param transform: image transform pipeline\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        assert self.split in {'TRAIN', 'VAL', 'TEST'}\n",
    "\n",
    "        # Open hdf5 file where images are stored\n",
    "        self.h = h5py.File(os.path.join(data_folder, self.split +  '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
    "        self.imgs = self.h['Images']\n",
    "\n",
    "        # captions per image \n",
    "        self.cpi = self.h.attrs['captions_per_image']\n",
    "\n",
    "        # Load encoded captions (completely into memory)\n",
    "        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n",
    "            self.captions = json.load(j)\n",
    "\n",
    "        # Load caption lengths (completely into memory)\n",
    "        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n",
    "            self.caplens = json.load(j)\n",
    "\n",
    "        # PyTorch transformation pipeline for the image (normalizing, etc.)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.captions)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Remember, the Nth caption corresponds to the ( N // captions_per_image)th image\n",
    "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
    "        \n",
    "        # Apply the transform to the image\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        caption = torch.LongTensor(self.captions[i])\n",
    "        \n",
    "        caplen = torch.LongTensor([self.caplens[i]])\n",
    "\n",
    "        if self.split is 'TRAIN':\n",
    "            return img, caption, caplen\n",
    "\n",
    "        else:\n",
    "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
    "            all_captions = torch.LongTensor(\n",
    "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
    "            return img, caption, caplen, all_captions\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data parameters\n",
    "data_folder = '/'  # folder with data files saved by create_input_files.py\n",
    "data_name = 'flickr8k_5_cap_per_img_5_min_word_freq'   # base name shared by data files\n",
    "\n",
    "# Model parameters\n",
    "emb_dim = 512  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Training parameters\n",
    "start_epoch = 13\n",
    "epochs = 40  # number of epochs to train for (if early stopping is not triggered)\n",
    "epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
    "batch_size = 256\n",
    "workers = 1  # for data-loading; right now, only 1 works with h5py\n",
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "grad_clip = 5.  # clip gradients at an absolute value of\n",
    "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "best_bleu4 = 0.  # BLEU-4 score right now\n",
    "print_freq = 100  # print training/validation stats every __ batches\n",
    "fine_tune_encoder = False  # fine-tune encoder\n",
    "\n",
    "# BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\n",
    "checkpoint = \"checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar\"  # path to checkpoint, None if none\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''Training and Validation'''\n",
    "\n",
    "    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n",
    "\n",
    "    # Read word map\n",
    "    word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
    "    with open(word_map_file, 'r') as j:\n",
    "        word_map = json.load(j)\n",
    "\n",
    "\n",
    "    # Initialize or load checkpoint\n",
    "    if checkpoint is None:\n",
    "        decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
    "                                       embed_dim=emb_dim,\n",
    "                                       decoder_dim=decoder_dim,\n",
    "                                       vocab_size=len(word_map),\n",
    "                                       dropout=dropout)\n",
    "        \n",
    "        decoder_optimizer = torch.optim.Adam(params = filter(lambda p: p.requires_grad, decoder.parameters()),lr=decoder_lr)\n",
    "\n",
    "        encoder = Encoder()\n",
    "        encoder.fine_tune(fine_tune_encoder)\n",
    "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                             lr=encoder_lr) if fine_tune_encoder else None\n",
    "\n",
    "    else:\n",
    "        # Loading the checkpoint\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "        if 'blue-4' in checkpoint:\n",
    "            best_bleu4 = checkpoint['blue-4']\n",
    "        else:\n",
    "            best_bleu4 = 0.0  # Assign a default value if the key is not present\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "        if fine_tune_encoder is True and encoder_optimizer is None:\n",
    "            encoder.fine_tune(fine_tune_encoder)\n",
    "            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                                 lr=encoder_lr)\n",
    "\n",
    "    # Move to GPU, if available\n",
    "    decoder = decoder.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Custom DataLoaders\n",
    "    # normalizing with args most suited for Resnet-101, check torchvision docs for more info.\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std =[0.229, 0.224, 0.225])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        # Custom dataset\n",
    "        CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True\n",
    "    ) #If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory.\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "\n",
    "    print(f'Training on {device}')\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
    "        if epochs_since_improvement == 20:\n",
    "            break\n",
    "        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
    "            adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "            # if encoder is being trained too then change its lr\n",
    "            if fine_tune_encoder:\n",
    "                adjust_learning_rate(encoder_optimizer, 0.8)\n",
    "\n",
    "        # One epoch's training\n",
    "        train(\n",
    "            train_loader=train_loader,\n",
    "            encoder = encoder,\n",
    "            decoder = decoder,\n",
    "            criterion = criterion,\n",
    "            encoder_optimizer = encoder_optimizer,\n",
    "            decoder_optimizer = decoder_optimizer,\n",
    "            epoch = epoch\n",
    "        )\n",
    "\n",
    "        # One epoch's validation\n",
    "        recent_bleu4 = validate(\n",
    "            val_loader=val_loader,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder,\n",
    "            criterion=criterion\n",
    "        )\n",
    "\n",
    "        # Check if there was an improvement\n",
    "        is_best = recent_bleu4 > best_bleu4\n",
    "        best_bleu4 = max(recent_bleu4, best_bleu4)\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        # save checkpoint\n",
    "        save_checkpoint(data_name,epoch,epochs_since_improvement,encoder,decoder,encoder_optimizer,\n",
    "                        decoder_optimizer, recent_bleu4, is_best)\n",
    "\n",
    "\n",
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
    "    '''\n",
    "    Performs one epoch of training\n",
    "\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param criterion: loss layer\n",
    "    :param  encoder_optimizer: optimizer to update encoder's weights ( if fine-tuning)\n",
    "    :param decoder_optimizer: optimizer to update decoder's weights\n",
    "    :param epoch: epoch number \n",
    "    '''\n",
    "\n",
    "    # Set both the models to train mode: This will enable all dropouts and batch_norm layers\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "\n",
    "    batch_time = AverageMeter() #forward prop + back prop time\n",
    "    data_time  = AverageMeter() # data loading time\n",
    "    losses = AverageMeter() # loss (per word decoded)\n",
    "    top5accs = AverageMeter() # top 5 accuracy\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches \n",
    "    for i, (imgs, caps, caplens) in tqdm(enumerate(train_loader)):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to GPU, if available\n",
    "        # imgs.size() = [batch_size, C, H, W] == [32,3,256,256]\n",
    "        imgs = imgs.to(device)\n",
    "        # caps.size() = [batch_size, max_caption_len] == [32,100] \n",
    "        caps = caps.to(device)\n",
    "        # caplens.size() = [batch_size, 1] == [32,1] \n",
    "        caplens = caplens.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        imgs = encoder(imgs)\n",
    "        # imgs.size() = [batch_size, 14,14,2048]\n",
    "\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
    "\n",
    "        # since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = caps_sorted[:, 1:]\n",
    "\n",
    "        # Remove timesteps that we didn't decode at or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        scores = pack_padded_sequence(scores, decode_lengths, batch_first = True)\n",
    "        scores = scores.data\n",
    "        targets= pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "        targets = targets.data\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += alpha_c*((1.-alphas.sum(dim=1)**2)).mean()\n",
    "\n",
    "        # Back prop\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(decoder_optimizer, grad_clip)\n",
    "            if encoder_optimizer is not None:\n",
    "                clip_gradient(encoder_optimizer, grad_clip)\n",
    "\n",
    "        # update weights\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "        # Keep track of metrics\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        \n",
    "\n",
    "        # Keep track of metrics\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
    "                                                                          batch_time=batch_time,\n",
    "                                                                  data_time=data_time, loss=losses,\n",
    "                                                                          top5=top5accs))\n",
    "\n",
    "def validate(val_loader, encoder, decoder, criterion):\n",
    "    \"\"\"\n",
    "    Performs one epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data.\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param criterion: loss layer\n",
    "    :return: BLEU-4 score\n",
    "    \"\"\"\n",
    "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
    "    if encoder is not None:\n",
    "        encoder.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top5accs = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
    "    hypotheses = list()  # hypotheses (predictions)\n",
    "\n",
    "    # explicitly disable gradient calculation to avoid CUDA memory error\n",
    "    # solves the issue #57\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (imgs, caps, caplens, allcaps) in tqdm(enumerate(val_loader)):\n",
    "\n",
    "            # Move to device, if available\n",
    "            imgs = imgs.to(device)\n",
    "            caps = caps.to(device)\n",
    "            caplens = caplens.to(device)\n",
    "\n",
    "            # Forward prop.\n",
    "            if encoder is not None:\n",
    "                imgs = encoder(imgs)\n",
    "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
    "\n",
    "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "            targets = caps_sorted[:, 1:]\n",
    "\n",
    "            # Remove timesteps that we didn't decode at, or are pads\n",
    "            # pack_padded_sequence is an easy trick to do this\n",
    "            scores_copy = scores.clone()\n",
    "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "            scores = scores.data\n",
    "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "            targets = targets.data\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(scores, targets)\n",
    "\n",
    "            # Add doubly stochastic attention regularization\n",
    "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "            # Keep track of metrics\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            top5 = accuracy(scores, targets, 5)\n",
    "            top5accs.update(top5, sum(decode_lengths))\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Validation: [{0}/{1}]\\t'\n",
    "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
    "                                                                                loss=losses, top5=top5accs))\n",
    "\n",
    "            # Store references (true captions), and hypothesis (prediction) for each image\n",
    "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
    "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
    "\n",
    "            # References\n",
    "            sort_ind = sort_ind.cpu()\n",
    "            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder \n",
    "            for j in range(allcaps.shape[0]):\n",
    "                img_caps = allcaps[j].tolist()\n",
    "                img_captions = list(\n",
    "                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
    "                        img_caps))  # remove <start> and pads\n",
    "                references.append(img_captions)\n",
    "\n",
    "            # Hypothesis\n",
    "            _, preds = torch.max(scores_copy, dim = 2)\n",
    "            preds = preds.tolist()\n",
    "            temp_preds = list()\n",
    "            for j, p in enumerate(preds):\n",
    "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
    "            preds = temp_preds\n",
    "            hypotheses.extend(preds)\n",
    "\n",
    "            assert len(references) == len(hypotheses)\n",
    "\n",
    "        # Calculate BLEU-4 scores\n",
    "        bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "\n",
    "        # Calculate BLEU-2 score\n",
    "        bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
    "\n",
    "        # Calculate BLEU-3 score\n",
    "        bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
    "\n",
    "        # Calculate BLEU-4 score\n",
    "        bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "        print(\n",
    "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-1 - {bleu}\\n'.format(\n",
    "                loss=losses,\n",
    "                top5=top5accs,\n",
    "                bleu=bleu1))\n",
    "        print(\n",
    "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-2 - {bleu}\\n'.format(\n",
    "                loss=losses,\n",
    "                top5=top5accs,\n",
    "                bleu=bleu2))\n",
    "        print(\n",
    "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-3 - {bleu}\\n'.format(\n",
    "                loss=losses,\n",
    "                top5=top5accs,\n",
    "                bleu=bleu3))\n",
    "        print(\n",
    "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
    "                loss=losses,\n",
    "                top5=top5accs,\n",
    "                bleu=bleu4))\n",
    "\n",
    "    return bleu4\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:04,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/118]\tBatch Time 4.381 (4.381)\tData Load Time 0.370 (0.370)\tLoss 2.6546 (2.6546)\tTop-5 Accuracy 74.475 (74.475)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:29,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][100/118]\tBatch Time 0.848 (0.883)\tData Load Time 0.002 (0.005)\tLoss 2.6846 (2.6332)\tTop-5 Accuracy 73.529 (73.890)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:43,  1.14it/s]\n",
      "1it [00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.958 (0.958)\tLoss 4.6399 (4.6399)\tTop-5 Accuracy 64.332 (64.332)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:14,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.607, TOP-5 ACCURACY - 65.732, BLEU-1 - 0.6303009977362287\n",
      "\n",
      "\n",
      " * LOSS - 4.607, TOP-5 ACCURACY - 65.732, BLEU-2 - 0.39232771761908947\n",
      "\n",
      "\n",
      " * LOSS - 4.607, TOP-5 ACCURACY - 65.732, BLEU-3 - 0.23179401325727497\n",
      "\n",
      "\n",
      " * LOSS - 4.607, TOP-5 ACCURACY - 65.732, BLEU-4 - 0.13155087056196907\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/118]\tBatch Time 1.126 (1.126)\tData Load Time 0.258 (0.258)\tLoss 2.5671 (2.5671)\tTop-5 Accuracy 74.478 (74.478)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][100/118]\tBatch Time 0.859 (0.853)\tData Load Time 0.001 (0.004)\tLoss 2.4389 (2.5633)\tTop-5 Accuracy 76.000 (74.988)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:39,  1.18it/s]\n",
      "1it [00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.900 (0.900)\tLoss 4.6636 (4.6636)\tTop-5 Accuracy 64.874 (64.874)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:12,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.609, TOP-5 ACCURACY - 65.849, BLEU-1 - 0.624130124926637\n",
      "\n",
      "\n",
      " * LOSS - 4.609, TOP-5 ACCURACY - 65.849, BLEU-2 - 0.3885252514008901\n",
      "\n",
      "\n",
      " * LOSS - 4.609, TOP-5 ACCURACY - 65.849, BLEU-3 - 0.22829300259512078\n",
      "\n",
      "\n",
      " * LOSS - 4.609, TOP-5 ACCURACY - 65.849, BLEU-4 - 0.12835879466972397\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/118]\tBatch Time 1.121 (1.121)\tData Load Time 0.259 (0.259)\tLoss 2.4787 (2.4787)\tTop-5 Accuracy 76.222 (76.222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][100/118]\tBatch Time 0.865 (0.853)\tData Load Time 0.001 (0.004)\tLoss 2.4668 (2.5023)\tTop-5 Accuracy 75.543 (75.905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:39,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.907 (0.907)\tLoss 4.6519 (4.6519)\tTop-5 Accuracy 66.026 (66.026)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:12,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.621, TOP-5 ACCURACY - 65.872, BLEU-1 - 0.6316592605013834\n",
      "\n",
      "\n",
      " * LOSS - 4.621, TOP-5 ACCURACY - 65.872, BLEU-2 - 0.39041775267797146\n",
      "\n",
      "\n",
      " * LOSS - 4.621, TOP-5 ACCURACY - 65.872, BLEU-3 - 0.2287228741754237\n",
      "\n",
      "\n",
      " * LOSS - 4.621, TOP-5 ACCURACY - 65.872, BLEU-4 - 0.12808683434863372\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/118]\tBatch Time 1.116 (1.116)\tData Load Time 0.258 (0.258)\tLoss 2.5358 (2.5358)\tTop-5 Accuracy 75.397 (75.397)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][100/118]\tBatch Time 0.864 (0.853)\tData Load Time 0.001 (0.004)\tLoss 2.4458 (2.4434)\tTop-5 Accuracy 76.375 (76.749)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.908 (0.908)\tLoss 4.5039 (4.5039)\tTop-5 Accuracy 67.516 (67.516)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:12,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.625, TOP-5 ACCURACY - 65.914, BLEU-1 - 0.6237779827282636\n",
      "\n",
      "\n",
      " * LOSS - 4.625, TOP-5 ACCURACY - 65.914, BLEU-2 - 0.3871053763567375\n",
      "\n",
      "\n",
      " * LOSS - 4.625, TOP-5 ACCURACY - 65.914, BLEU-3 - 0.228062993133732\n",
      "\n",
      "\n",
      " * LOSS - 4.625, TOP-5 ACCURACY - 65.914, BLEU-4 - 0.12938992306915584\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/118]\tBatch Time 1.118 (1.118)\tData Load Time 0.259 (0.259)\tLoss 2.3659 (2.3659)\tTop-5 Accuracy 78.726 (78.726)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][100/118]\tBatch Time 0.854 (0.854)\tData Load Time 0.002 (0.004)\tLoss 2.3914 (2.3888)\tTop-5 Accuracy 78.252 (77.616)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.912 (0.912)\tLoss 4.6360 (4.6360)\tTop-5 Accuracy 65.543 (65.543)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:13,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.624, TOP-5 ACCURACY - 65.817, BLEU-1 - 0.6273329420642241\n",
      "\n",
      "\n",
      " * LOSS - 4.624, TOP-5 ACCURACY - 65.817, BLEU-2 - 0.3919305968311347\n",
      "\n",
      "\n",
      " * LOSS - 4.624, TOP-5 ACCURACY - 65.817, BLEU-3 - 0.2334746676727085\n",
      "\n",
      "\n",
      " * LOSS - 4.624, TOP-5 ACCURACY - 65.817, BLEU-4 - 0.13440184784709922\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/118]\tBatch Time 1.110 (1.110)\tData Load Time 0.259 (0.259)\tLoss 2.3053 (2.3053)\tTop-5 Accuracy 79.067 (79.067)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][100/118]\tBatch Time 0.844 (0.854)\tData Load Time 0.001 (0.004)\tLoss 2.3771 (2.3298)\tTop-5 Accuracy 78.800 (78.443)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.925 (0.925)\tLoss 4.6001 (4.6001)\tTop-5 Accuracy 66.402 (66.402)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:12,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.634, TOP-5 ACCURACY - 65.772, BLEU-1 - 0.6270478745703026\n",
      "\n",
      "\n",
      " * LOSS - 4.634, TOP-5 ACCURACY - 65.772, BLEU-2 - 0.39028608081713856\n",
      "\n",
      "\n",
      " * LOSS - 4.634, TOP-5 ACCURACY - 65.772, BLEU-3 - 0.23065939462282833\n",
      "\n",
      "\n",
      " * LOSS - 4.634, TOP-5 ACCURACY - 65.772, BLEU-4 - 0.13091649237206895\n",
      "\n",
      "\n",
      "Epochs since last improvement: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/118]\tBatch Time 1.135 (1.135)\tData Load Time 0.257 (0.257)\tLoss 2.2230 (2.2230)\tTop-5 Accuracy 80.498 (80.498)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][100/118]\tBatch Time 0.857 (0.855)\tData Load Time 0.001 (0.004)\tLoss 2.2916 (2.2809)\tTop-5 Accuracy 79.237 (79.256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.911 (0.911)\tLoss 4.6791 (4.6791)\tTop-5 Accuracy 64.905 (64.905)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:13,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.648, TOP-5 ACCURACY - 65.643, BLEU-1 - 0.6218495849752662\n",
      "\n",
      "\n",
      " * LOSS - 4.648, TOP-5 ACCURACY - 65.643, BLEU-2 - 0.38455806151192623\n",
      "\n",
      "\n",
      " * LOSS - 4.648, TOP-5 ACCURACY - 65.643, BLEU-3 - 0.22657164513288564\n",
      "\n",
      "\n",
      " * LOSS - 4.648, TOP-5 ACCURACY - 65.643, BLEU-4 - 0.1269606455705955\n",
      "\n",
      "\n",
      "Epochs since last improvement: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/118]\tBatch Time 1.118 (1.118)\tData Load Time 0.259 (0.259)\tLoss 2.2788 (2.2788)\tTop-5 Accuracy 79.712 (79.712)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][100/118]\tBatch Time 0.863 (0.855)\tData Load Time 0.002 (0.004)\tLoss 2.3059 (2.2336)\tTop-5 Accuracy 79.039 (80.041)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.912 (0.912)\tLoss 4.6745 (4.6745)\tTop-5 Accuracy 65.770 (65.770)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:12,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.678, TOP-5 ACCURACY - 65.514, BLEU-1 - 0.6195522763477823\n",
      "\n",
      "\n",
      " * LOSS - 4.678, TOP-5 ACCURACY - 65.514, BLEU-2 - 0.3826635360169514\n",
      "\n",
      "\n",
      " * LOSS - 4.678, TOP-5 ACCURACY - 65.514, BLEU-3 - 0.22322787798357405\n",
      "\n",
      "\n",
      " * LOSS - 4.678, TOP-5 ACCURACY - 65.514, BLEU-4 - 0.12547839768656766\n",
      "\n",
      "\n",
      "Epochs since last improvement: 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/118]\tBatch Time 1.129 (1.129)\tData Load Time 0.262 (0.262)\tLoss 2.2033 (2.2033)\tTop-5 Accuracy 79.642 (79.642)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][100/118]\tBatch Time 0.851 (0.855)\tData Load Time 0.001 (0.004)\tLoss 2.2216 (2.1827)\tTop-5 Accuracy 80.792 (80.740)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.912 (0.912)\tLoss 4.6899 (4.6899)\tTop-5 Accuracy 65.868 (65.868)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:12,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.685, TOP-5 ACCURACY - 65.435, BLEU-1 - 0.6221681898214136\n",
      "\n",
      "\n",
      " * LOSS - 4.685, TOP-5 ACCURACY - 65.435, BLEU-2 - 0.38623717817516406\n",
      "\n",
      "\n",
      " * LOSS - 4.685, TOP-5 ACCURACY - 65.435, BLEU-3 - 0.22748498805870537\n",
      "\n",
      "\n",
      " * LOSS - 4.685, TOP-5 ACCURACY - 65.435, BLEU-4 - 0.12828407286970478\n",
      "\n",
      "\n",
      "Epochs since last improvement: 4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/118]\tBatch Time 1.119 (1.119)\tData Load Time 0.262 (0.262)\tLoss 2.1081 (2.1081)\tTop-5 Accuracy 81.645 (81.645)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][100/118]\tBatch Time 0.864 (0.855)\tData Load Time 0.001 (0.004)\tLoss 2.1881 (2.1403)\tTop-5 Accuracy 79.634 (81.431)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.908 (0.908)\tLoss 4.7455 (4.7455)\tTop-5 Accuracy 64.279 (64.279)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:12,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.681, TOP-5 ACCURACY - 65.289, BLEU-1 - 0.6174729605097677\n",
      "\n",
      "\n",
      " * LOSS - 4.681, TOP-5 ACCURACY - 65.289, BLEU-2 - 0.3831286580402354\n",
      "\n",
      "\n",
      " * LOSS - 4.681, TOP-5 ACCURACY - 65.289, BLEU-3 - 0.22489061319806478\n",
      "\n",
      "\n",
      " * LOSS - 4.681, TOP-5 ACCURACY - 65.289, BLEU-4 - 0.12802410523847396\n",
      "\n",
      "\n",
      "Epochs since last improvement: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/118]\tBatch Time 1.126 (1.126)\tData Load Time 0.263 (0.263)\tLoss 2.0744 (2.0744)\tTop-5 Accuracy 81.737 (81.737)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][100/118]\tBatch Time 0.877 (0.854)\tData Load Time 0.001 (0.004)\tLoss 2.1018 (2.0905)\tTop-5 Accuracy 81.983 (82.181)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.913 (0.913)\tLoss 4.7219 (4.7219)\tTop-5 Accuracy 65.591 (65.591)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:13,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.714, TOP-5 ACCURACY - 65.311, BLEU-1 - 0.6232246164165339\n",
      "\n",
      "\n",
      " * LOSS - 4.714, TOP-5 ACCURACY - 65.311, BLEU-2 - 0.3828138904666386\n",
      "\n",
      "\n",
      " * LOSS - 4.714, TOP-5 ACCURACY - 65.311, BLEU-3 - 0.2237551290404094\n",
      "\n",
      "\n",
      " * LOSS - 4.714, TOP-5 ACCURACY - 65.311, BLEU-4 - 0.1257631906986524\n",
      "\n",
      "\n",
      "Epochs since last improvement: 6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/118]\tBatch Time 1.128 (1.128)\tData Load Time 0.261 (0.261)\tLoss 2.0169 (2.0169)\tTop-5 Accuracy 83.289 (83.289)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][100/118]\tBatch Time 0.852 (0.854)\tData Load Time 0.002 (0.004)\tLoss 1.9824 (2.0532)\tTop-5 Accuracy 83.453 (82.798)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.905 (0.905)\tLoss 4.7329 (4.7329)\tTop-5 Accuracy 64.191 (64.191)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:13,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.709, TOP-5 ACCURACY - 65.401, BLEU-1 - 0.6197702691372516\n",
      "\n",
      "\n",
      " * LOSS - 4.709, TOP-5 ACCURACY - 65.401, BLEU-2 - 0.38152857567949006\n",
      "\n",
      "\n",
      " * LOSS - 4.709, TOP-5 ACCURACY - 65.401, BLEU-3 - 0.2249727066239365\n",
      "\n",
      "\n",
      " * LOSS - 4.709, TOP-5 ACCURACY - 65.401, BLEU-4 - 0.1279723431039212\n",
      "\n",
      "\n",
      "Epochs since last improvement: 7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/118]\tBatch Time 1.131 (1.131)\tData Load Time 0.260 (0.260)\tLoss 1.9642 (1.9642)\tTop-5 Accuracy 84.418 (84.418)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][100/118]\tBatch Time 0.867 (0.855)\tData Load Time 0.001 (0.004)\tLoss 1.9035 (2.0126)\tTop-5 Accuracy 84.351 (83.439)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.915 (0.915)\tLoss 4.7956 (4.7956)\tTop-5 Accuracy 64.985 (64.985)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:13,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.723, TOP-5 ACCURACY - 65.146, BLEU-1 - 0.6214303680724407\n",
      "\n",
      "\n",
      " * LOSS - 4.723, TOP-5 ACCURACY - 65.146, BLEU-2 - 0.38138365488089754\n",
      "\n",
      "\n",
      " * LOSS - 4.723, TOP-5 ACCURACY - 65.146, BLEU-3 - 0.22346679488911658\n",
      "\n",
      "\n",
      " * LOSS - 4.723, TOP-5 ACCURACY - 65.146, BLEU-4 - 0.1255499555019296\n",
      "\n",
      "\n",
      "Epochs since last improvement: 8\n",
      "\n",
      "\n",
      "DECAYING learning rate.\n",
      "The new learning rate is 0.000320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/118]\tBatch Time 1.132 (1.132)\tData Load Time 0.264 (0.264)\tLoss 1.9626 (1.9626)\tTop-5 Accuracy 84.436 (84.436)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][100/118]\tBatch Time 0.841 (0.855)\tData Load Time 0.002 (0.004)\tLoss 1.9368 (1.9434)\tTop-5 Accuracy 85.139 (84.470)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.908 (0.908)\tLoss 4.7342 (4.7342)\tTop-5 Accuracy 65.206 (65.206)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:13,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.755, TOP-5 ACCURACY - 65.265, BLEU-1 - 0.6195522763477823\n",
      "\n",
      "\n",
      " * LOSS - 4.755, TOP-5 ACCURACY - 65.265, BLEU-2 - 0.3811492068689056\n",
      "\n",
      "\n",
      " * LOSS - 4.755, TOP-5 ACCURACY - 65.265, BLEU-3 - 0.22229723401798296\n",
      "\n",
      "\n",
      " * LOSS - 4.755, TOP-5 ACCURACY - 65.265, BLEU-4 - 0.12541101785798503\n",
      "\n",
      "\n",
      "Epochs since last improvement: 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/118]\tBatch Time 1.138 (1.138)\tData Load Time 0.264 (0.264)\tLoss 1.8378 (1.8378)\tTop-5 Accuracy 85.110 (85.110)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][100/118]\tBatch Time 0.862 (0.854)\tData Load Time 0.002 (0.004)\tLoss 1.9579 (1.9079)\tTop-5 Accuracy 85.103 (85.102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.914 (0.914)\tLoss 4.6275 (4.6275)\tTop-5 Accuracy 66.720 (66.720)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:12,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.758, TOP-5 ACCURACY - 65.024, BLEU-1 - 0.616902825521925\n",
      "\n",
      "\n",
      " * LOSS - 4.758, TOP-5 ACCURACY - 65.024, BLEU-2 - 0.3788907558666818\n",
      "\n",
      "\n",
      " * LOSS - 4.758, TOP-5 ACCURACY - 65.024, BLEU-3 - 0.2207524926275044\n",
      "\n",
      "\n",
      " * LOSS - 4.758, TOP-5 ACCURACY - 65.024, BLEU-4 - 0.12466121486132899\n",
      "\n",
      "\n",
      "Epochs since last improvement: 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/118]\tBatch Time 1.129 (1.129)\tData Load Time 0.260 (0.260)\tLoss 1.8333 (1.8333)\tTop-5 Accuracy 84.603 (84.603)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][100/118]\tBatch Time 0.858 (0.854)\tData Load Time 0.001 (0.004)\tLoss 1.8793 (1.8728)\tTop-5 Accuracy 85.942 (85.614)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.908 (0.908)\tLoss 4.9214 (4.9214)\tTop-5 Accuracy 62.808 (62.808)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:13,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * LOSS - 4.768, TOP-5 ACCURACY - 64.741, BLEU-1 - 0.616047623040161\n",
      "\n",
      "\n",
      " * LOSS - 4.768, TOP-5 ACCURACY - 64.741, BLEU-2 - 0.3776887920378701\n",
      "\n",
      "\n",
      " * LOSS - 4.768, TOP-5 ACCURACY - 64.741, BLEU-3 - 0.22026958390137613\n",
      "\n",
      "\n",
      " * LOSS - 4.768, TOP-5 ACCURACY - 64.741, BLEU-4 - 0.12379684452494365\n",
      "\n",
      "\n",
      "Epochs since last improvement: 11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/118]\tBatch Time 1.141 (1.141)\tData Load Time 0.262 (0.262)\tLoss 1.7724 (1.7724)\tTop-5 Accuracy 86.964 (86.964)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [01:26,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][100/118]\tBatch Time 0.841 (0.854)\tData Load Time 0.002 (0.004)\tLoss 1.9237 (1.8390)\tTop-5 Accuracy 84.758 (86.060)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [01:40,  1.18it/s]\n",
      "1it [00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [0/20]\tBatch Time 0.910 (0.910)\tLoss 4.6733 (4.6733)\tTop-5 Accuracy 65.374 (65.374)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:06,  1.52it/s]"
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters\n",
    "data_folder = 'data_output'  # folder with data files saved by create_input_files.py\n",
    "data_name = 'flickr8k_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
    "checkpoint = 'BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.pth.tar'  # model checkpoint\n",
    "word_map_file = 'WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json'  # word map, ensure it's the same the data was encoded with and the model was trained with\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load(checkpoint)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "# Load word map (word2ix)\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}\n",
    "vocab_size = len(word_map)\n",
    "\n",
    "# Normalization transform\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def evaluate(beam_size):\n",
    "    \"\"\"\n",
    "    Evaluation\n",
    "    :param beam_size: beam size at which to generate captions for evaluation\n",
    "    :return: BLEU-4 score\n",
    "    \"\"\"\n",
    "    # DataLoader\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n",
    "        batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "    # TODO: Batched Beam Search\n",
    "    # Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n",
    "\n",
    "    # Lists to store references (true captions), and hypothesis (prediction) for each image\n",
    "    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
    "    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
    "    references = list()\n",
    "    hypotheses = list()\n",
    "\n",
    "    # For each image\n",
    "    for i, (image, caps, caplens, allcaps) in enumerate(\n",
    "            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
    "\n",
    "        k = beam_size\n",
    "\n",
    "        # Move to GPU device, if available\n",
    "        image = image.to(device)  # (1, 3, 256, 256)\n",
    "\n",
    "        # Encode\n",
    "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "        enc_image_size = encoder_out.size(1)\n",
    "        encoder_dim = encoder_out.size(3)\n",
    "\n",
    "        # Flatten encoding\n",
    "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # We'll treat the problem as having a batch size of k\n",
    "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "        # Tensor to store top k previous words at each step; now they're just <start>\n",
    "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "\n",
    "        # Tensor to store top k sequences; now they're just <start>\n",
    "        seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "        # Tensor to store top k sequences' scores; now they're just 0\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "        # Lists to store completed sequences and scores\n",
    "        complete_seqs = list()\n",
    "        complete_seqs_scores = list()\n",
    "\n",
    "        # Start decoding\n",
    "        step = 1\n",
    "        h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "        while True:\n",
    "\n",
    "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "            awe = gate * awe\n",
    "\n",
    "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "            scores = decoder.fc(h)  # (s, vocab_size)\n",
    "            scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "            # Add\n",
    "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "            if step == 1:\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "            else:\n",
    "                # Unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "            # Convert unrolled indices to actual indices of scores\n",
    "            prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "            next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "            # Add new words to sequences\n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "            # Which sequences are incomplete (didn't reach <end>)?\n",
    "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                               next_word != word_map['<end>']]\n",
    "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "            # Set aside complete sequences\n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "            # Proceed with incomplete sequences\n",
    "            if k == 0:\n",
    "                break\n",
    "            seqs = seqs[incomplete_inds]\n",
    "            h = h[prev_word_inds[incomplete_inds]]\n",
    "            c = c[prev_word_inds[incomplete_inds]]\n",
    "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "            # Break if things have been going on too long\n",
    "            if step > 50:\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        seq = complete_seqs[i]\n",
    "\n",
    "        # References\n",
    "        img_caps = allcaps[0].tolist()\n",
    "        img_captions = list(\n",
    "            map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n",
    "                img_caps))  # remove <start> and pads\n",
    "        references.append(img_captions)\n",
    "\n",
    "        # Hypotheses\n",
    "        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
    "\n",
    "        assert len(references) == len(hypotheses)\n",
    "\n",
    "    # Calculate BLEU-4 scores\n",
    "    bleu4 = corpus_bleu(references, hypotheses)\n",
    "\n",
    "    return bleu4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 2\n",
    "print(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (beam_size, evaluate(beam_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
