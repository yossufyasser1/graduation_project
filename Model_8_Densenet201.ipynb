{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d054240b",
   "metadata": {
    "id": "d054240b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-process images with ResNet521 weights ported from VQA 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "DvW051aCj12e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvW051aCj12e",
    "outputId": "3cb0386f-97ee-4312-9d58-f51feb820f87",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Visual-question-answering-model'...\n",
      "remote: Enumerating objects: 55, done.\u001b[K\n",
      "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
      "remote: Total 55 (delta 8), reused 21 (delta 7), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (55/55), 4.21 MiB | 8.16 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yossufyasser1/Visual-question-answering-model.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "A8nRW4wzj7p6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8nRW4wzj7p6",
    "outputId": "84e126e0-9c15-40fe-c33c-c77ba535f798",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Visual-question-answering-model\n"
     ]
    }
   ],
   "source": [
    "%cd /Visual-question-answering-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb79be0-7987-4924-8a91-167d5f6bb3f9",
   "metadata": {
    "id": "8eb79be0-7987-4924-8a91-167d5f6bb3f9",
    "outputId": "d7417bc5-dbbd-44c9-8074-ecc2252f2688",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.65.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.5.9-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.6.3)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.24,>=1.22\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.13,>=2.12.0\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jax>=0.3.15\n",
      "  Downloading jax-0.4.10.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.23.1-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.13,>=2.12\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Downloading ml_dtypes-0.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.29.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.18.1-py2.py3-none-any.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: efficientnet_pytorch, jax\n",
      "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=65cd1f9fe61ae932bdabb2561b1c9dbf45ce3b0d6c840430b580851c05d0094c\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
      "  Building wheel for jax (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.4.10-py3-none-any.whl size=1480503 sha256=dbcec15fbf3884fb3b0c42df6f1a735c7e89f4745bb771e57b9f8da4c97d05e7\n",
      "  Stored in directory: /root/.cache/pip/wheels/2f/04/51/ebc9c5225f0a0df1e56c231c1f4c9b7afd3e024ebb492eed99\n",
      "Successfully built efficientnet_pytorch jax\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, markdown, keras, grpcio, google-pasta, gast, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, opt-einsum, ml-dtypes, h5py, jax, google-auth, efficientnet_pytorch, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Successfully installed absl-py-1.4.0 cachetools-5.3.0 efficientnet_pytorch-0.7.1 flatbuffers-23.5.9 gast-0.4.0 google-auth-2.18.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.2 h5py-3.8.0 jax-0.4.10 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.1.0 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.1 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.3 tensorboard-data-server-0.7.0 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.4 wrapt-1.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch tqdm tensorflow  h5py efficientnet_pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vWcOZ9orkFAI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "vWcOZ9orkFAI",
    "outputId": "916e676a-1008-49f0-b3ab-13eae630f3f2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 01:24:15.138289: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-23 01:24:16.661897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "import data\n",
    "import utils\n",
    "from resnet import resnet as caffe_resnet\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import config\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3748a7f-c579-4faf-9b13-197aa083d0ec",
   "metadata": {
    "id": "a3748a7f-c579-4faf-9b13-197aa083d0ec",
    "outputId": "c9408510-3336-4af6-93c8-b4a6a51da697",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 1 not upgraded.\n",
      "Need to get 168 kB of archives.\n",
      "After this operation, 593 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 unzip amd64 6.0-25ubuntu1.1 [168 kB]\n",
      "Fetched 168 kB in 1s (264 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package unzip.\n",
      "(Reading database ... 19436 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-25ubuntu1.1_amd64.deb ...\n",
      "Unpacking unzip (6.0-25ubuntu1.1) ...\n",
      "Setting up unzip (6.0-25ubuntu1.1) ...\n",
      "Processing triggers for mime-support (3.64ubuntu1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "t_WQKWUqrdd0",
   "metadata": {
    "id": "t_WQKWUqrdd0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip\n",
      "7239401/7239401 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "_URL = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip'\n",
    "zip_dir = tf.keras.utils.get_file('/QUEStrain2014.zip', origin=_URL, extract=False,archive_format='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c660c1-c127-4ea0-b335-1bae8ce1ec32",
   "metadata": {
    "id": "b6c660c1-c127-4ea0-b335-1bae8ce1ec32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fname = '/QUEStrain2014.zip'\n",
    "!unzip -q $fname -d /Visual-question-answering-model/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0iRMp5iBsBai",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iRMp5iBsBai",
    "outputId": "071a36ed-f47e-4a3e-906c-9049f5a71d70",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
      "13510573713/13510573713 [==============================] - 673s 0us/step\n"
     ]
    }
   ],
   "source": [
    "_URL = 'http://images.cocodataset.org/zips/train2014.zip'\n",
    "zip_dir = tf.keras.utils.get_file('/MSCOCOTRAIN2014.zip', origin=_URL, extract=False,archive_format='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9ff50a1-71c0-4ab3-a2ca-3b0991623b9b",
   "metadata": {
    "id": "b9ff50a1-71c0-4ab3-a2ca-3b0991623b9b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fname = '/MSCOCOTRAIN2014.zip'\n",
    "!unzip -q $fname -d /Visual-question-answering-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1wTWY44EvEks",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wTWY44EvEks",
    "outputId": "8b4bfc40-8c22-4864-8fb1-68cbe4b2a713",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://images.cocodataset.org/zips/val2014.zip\n",
      "6645013297/6645013297 [==============================] - 365s 0us/step\n"
     ]
    }
   ],
   "source": [
    "_URL = 'http://images.cocodataset.org/zips/val2014.zip'\n",
    "zip_dir = tf.keras.utils.get_file('/MSCOCOVAL12014.zip', origin=_URL, extract=False,archive_format='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74c3ae62-c458-4f00-ad90-9305f86fc0ec",
   "metadata": {
    "id": "74c3ae62-c458-4f00-ad90-9305f86fc0ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fname = '/MSCOCOVAL12014.zip'\n",
    "!unzip -q $fname -d /Visual-question-answering-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a69ad260-e4ca-45ee-8030-dd4ba0f73e8a",
   "metadata": {
    "id": "a69ad260-e4ca-45ee-8030-dd4ba0f73e8a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\n",
      "3494929/3494929 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "_URL = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip'\n",
    "zip_dir = tf.keras.utils.get_file('/QUESVAL2014.zip', origin=_URL, extract=False,archive_format='auto')\n",
    "fname = '/QUESVAL2014.zip'\n",
    "!unzip -q $fname -d /Visual-question-answering-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7adca965-b4e7-4d43-97c8-346105300858",
   "metadata": {
    "id": "7adca965-b4e7-4d43-97c8-346105300858",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\n",
      "10518930/10518930 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "_URL = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip'\n",
    "zip_dir = tf.keras.utils.get_file('/ANNOTVAL2014.zip', origin=_URL, extract=False,archive_format='auto')\n",
    "fname = '/ANNOTVAL2014.zip'\n",
    "!unzip -q $fname -d /Visual-question-answering-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81be435e-dd53-43e4-a1e9-4018c0fbfb9d",
   "metadata": {
    "id": "81be435e-dd53-43e4-a1e9-4018c0fbfb9d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/snagiri/ECE285_Jarvis_ProjectA/releases/download/v1.0/50epoch.pth\n",
      "276298398/276298398 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "_URL = 'https://github.com/snagiri/ECE285_Jarvis_ProjectA/releases/download/v1.0/50epoch.pth'\n",
    "zip_dir = tf.keras.utils.get_file('/Visual-question-answering-model/MSCOCOVAL2014.pth', origin=_URL, extract=False,archive_format='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35615b18-285f-4b9d-94aa-d5c9cf4f0257",
   "metadata": {
    "id": "35615b18-285f-4b9d-94aa-d5c9cf4f0257",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip\n",
      "21708861/21708861 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "_URL = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip'\n",
    "zip_dir = tf.keras.utils.get_file('/ANNOTTrain2014.zip', origin=_URL, extract=False,archive_format='auto')\n",
    "fname = '/ANNOTTrain2014.zip'\n",
    "!unzip -q $fname -d /Visual-question-answering-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d5754ce-34f9-4e01-9bdc-0fdd7d9e8591",
   "metadata": {
    "id": "0d5754ce-34f9-4e01-9bdc-0fdd7d9e8591",
    "outputId": "835680dd-2d3d-4aad-dae7-f66bf34e1f6b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu cpp cpp-9 dirmngr\n",
      "  dpkg-dev fakeroot g++ g++-9 gcc gcc-9 gcc-9-base gnupg gnupg-l10n\n",
      "  gnupg-utils gpg-agent gpg-wks-client gpg-wks-server gpgsm\n",
      "  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n",
      "  libasan5 libatomic1 libbinutils libcc1-0 libctf-nobfd0 libctf0 libdpkg-perl\n",
      "  libfakeroot libfile-fcntllock-perl libgcc-9-dev libisl22 libitm1 libksba8\n",
      "  liblocale-gettext-perl liblsan0 libmpc3 libmpfr6 libnpth0 libquadmath0\n",
      "  libstdc++-9-dev libtsan0 libubsan1 make patch pinentry-curses xz-utils\n",
      "Suggested packages:\n",
      "  binutils-doc cpp-doc gcc-9-locales pinentry-gnome3 tor debian-keyring\n",
      "  g++-multilib g++-9-multilib gcc-9-doc gcc-multilib manpages-dev autoconf\n",
      "  automake libtool flex bison gdb gcc-doc gcc-9-multilib parcimonie xloadimage\n",
      "  scdaemon bzr libstdc++-9-doc make-doc ed diffutils-doc pinentry-doc\n",
      "The following NEW packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu build-essential cpp cpp-9\n",
      "  dirmngr dpkg-dev fakeroot g++ g++-9 gcc gcc-9 gcc-9-base gnupg gnupg-l10n\n",
      "  gnupg-utils gpg-agent gpg-wks-client gpg-wks-server gpgsm\n",
      "  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n",
      "  libasan5 libatomic1 libbinutils libcc1-0 libctf-nobfd0 libctf0 libdpkg-perl\n",
      "  libfakeroot libfile-fcntllock-perl libgcc-9-dev libisl22 libitm1 libksba8\n",
      "  liblocale-gettext-perl liblsan0 libmpc3 libmpfr6 libnpth0 libquadmath0\n",
      "  libstdc++-9-dev libtsan0 libubsan1 make patch pinentry-curses xz-utils\n",
      "0 upgraded, 50 newly installed, 0 to remove and 1 not upgraded.\n",
      "Need to get 41.6 MB of archives.\n",
      "After this operation, 179 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 liblocale-gettext-perl amd64 1.07-4 [17.1 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xz-utils amd64 5.2.4-1ubuntu1.1 [82.6 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 binutils-common amd64 2.34-6ubuntu1.4 [207 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libbinutils amd64 2.34-6ubuntu1.4 [474 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libctf-nobfd0 amd64 2.34-6ubuntu1.4 [47.2 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libctf0 amd64 2.34-6ubuntu1.4 [46.6 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.34-6ubuntu1.4 [1613 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 binutils amd64 2.34-6ubuntu1.4 [3380 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gcc-9-base amd64 9.4.0-1ubuntu1~20.04.1 [19.4 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 libisl22 amd64 0.22.1-1 [592 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libmpfr6 amd64 4.0.2-1 [240 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libmpc3 amd64 1.1.0-1 [40.8 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 cpp-9 amd64 9.4.0-1ubuntu1~20.04.1 [7500 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 cpp amd64 4:9.3.0-1ubuntu2 [27.6 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libcc1-0 amd64 10.3.0-1ubuntu1~20.04 [48.8 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libitm1 amd64 10.3.0-1ubuntu1~20.04 [26.2 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libatomic1 amd64 10.3.0-1ubuntu1~20.04 [9284 B]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libasan5 amd64 9.4.0-1ubuntu1~20.04.1 [2751 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 liblsan0 amd64 10.3.0-1ubuntu1~20.04 [835 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libtsan0 amd64 10.3.0-1ubuntu1~20.04 [2009 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libubsan1 amd64 10.3.0-1ubuntu1~20.04 [784 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libquadmath0 amd64 10.3.0-1ubuntu1~20.04 [146 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgcc-9-dev amd64 9.4.0-1ubuntu1~20.04.1 [2359 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gcc-9 amd64 9.4.0-1ubuntu1~20.04.1 [8274 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu focal/main amd64 gcc amd64 4:9.3.0-1ubuntu2 [5208 B]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libstdc++-9-dev amd64 9.4.0-1ubuntu1~20.04.1 [1722 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 g++-9 amd64 9.4.0-1ubuntu1~20.04.1 [8420 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu focal/main amd64 g++ amd64 4:9.3.0-1ubuntu2 [1604 B]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu focal/main amd64 make amd64 4.2.1-1.2 [162 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libdpkg-perl all 1.19.7ubuntu3.2 [231 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu focal/main amd64 patch amd64 2.7.6-6 [105 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 dpkg-dev all 1.19.7ubuntu3.2 [679 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 build-essential amd64 12.8ubuntu1.1 [4664 B]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libksba8 amd64 1.3.5-2ubuntu0.20.04.2 [95.2 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu focal/main amd64 libnpth0 amd64 1.6-1 [7736 B]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 dirmngr amd64 2.2.19-3ubuntu2.2 [330 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu focal/main amd64 libfakeroot amd64 1.24-1 [25.7 kB]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu focal/main amd64 fakeroot amd64 1.24-1 [62.6 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gnupg-l10n all 2.2.19-3ubuntu2.2 [51.7 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gnupg-utils amd64 2.2.19-3ubuntu2.2 [481 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu focal/main amd64 pinentry-curses amd64 1.1.0-3build1 [36.3 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gpg-agent amd64 2.2.19-3ubuntu2.2 [232 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gpg-wks-client amd64 2.2.19-3ubuntu2.2 [97.4 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gpg-wks-server amd64 2.2.19-3ubuntu2.2 [90.2 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gpgsm amd64 2.2.19-3ubuntu2.2 [217 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 gnupg all 2.2.19-3ubuntu2.2 [259 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu focal/main amd64 libalgorithm-diff-perl all 1.19.03-2 [46.6 kB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu focal/main amd64 libalgorithm-diff-xs-perl amd64 0.04-6 [11.3 kB]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu focal/main amd64 libalgorithm-merge-perl all 0.08-3 [12.0 kB]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu focal/main amd64 libfile-fcntllock-perl amd64 0.22-3build4 [33.1 kB]\n",
      "Fetched 41.6 MB in 2s (18.8 MB/s)                 \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package liblocale-gettext-perl.\n",
      "(Reading database ... 19454 files and directories currently installed.)\n",
      "Preparing to unpack .../00-liblocale-gettext-perl_1.07-4_amd64.deb ...\n",
      "Unpacking liblocale-gettext-perl (1.07-4) ...\n",
      "Selecting previously unselected package xz-utils.\n",
      "Preparing to unpack .../01-xz-utils_5.2.4-1ubuntu1.1_amd64.deb ...\n",
      "Unpacking xz-utils (5.2.4-1ubuntu1.1) ...\n",
      "Selecting previously unselected package binutils-common:amd64.\n",
      "Preparing to unpack .../02-binutils-common_2.34-6ubuntu1.4_amd64.deb ...\n",
      "Unpacking binutils-common:amd64 (2.34-6ubuntu1.4) ...\n",
      "Selecting previously unselected package libbinutils:amd64.\n",
      "Preparing to unpack .../03-libbinutils_2.34-6ubuntu1.4_amd64.deb ...\n",
      "Unpacking libbinutils:amd64 (2.34-6ubuntu1.4) ...\n",
      "Selecting previously unselected package libctf-nobfd0:amd64.\n",
      "Preparing to unpack .../04-libctf-nobfd0_2.34-6ubuntu1.4_amd64.deb ...\n",
      "Unpacking libctf-nobfd0:amd64 (2.34-6ubuntu1.4) ...\n",
      "Selecting previously unselected package libctf0:amd64.\n",
      "Preparing to unpack .../05-libctf0_2.34-6ubuntu1.4_amd64.deb ...\n",
      "Unpacking libctf0:amd64 (2.34-6ubuntu1.4) ...\n",
      "Selecting previously unselected package binutils-x86-64-linux-gnu.\n",
      "Preparing to unpack .../06-binutils-x86-64-linux-gnu_2.34-6ubuntu1.4_amd64.deb ...\n",
      "Unpacking binutils-x86-64-linux-gnu (2.34-6ubuntu1.4) ...\n",
      "Selecting previously unselected package binutils.\n",
      "Preparing to unpack .../07-binutils_2.34-6ubuntu1.4_amd64.deb ...\n",
      "Unpacking binutils (2.34-6ubuntu1.4) ...\n",
      "Selecting previously unselected package gcc-9-base:amd64.\n",
      "Preparing to unpack .../08-gcc-9-base_9.4.0-1ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking gcc-9-base:amd64 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package libisl22:amd64.\n",
      "Preparing to unpack .../09-libisl22_0.22.1-1_amd64.deb ...\n",
      "Unpacking libisl22:amd64 (0.22.1-1) ...\n",
      "Selecting previously unselected package libmpfr6:amd64.\n",
      "Preparing to unpack .../10-libmpfr6_4.0.2-1_amd64.deb ...\n",
      "Unpacking libmpfr6:amd64 (4.0.2-1) ...\n",
      "Selecting previously unselected package libmpc3:amd64.\n",
      "Preparing to unpack .../11-libmpc3_1.1.0-1_amd64.deb ...\n",
      "Unpacking libmpc3:amd64 (1.1.0-1) ...\n",
      "Selecting previously unselected package cpp-9.\n",
      "Preparing to unpack .../12-cpp-9_9.4.0-1ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking cpp-9 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package cpp.\n",
      "Preparing to unpack .../13-cpp_4%3a9.3.0-1ubuntu2_amd64.deb ...\n",
      "Unpacking cpp (4:9.3.0-1ubuntu2) ...\n",
      "Selecting previously unselected package libcc1-0:amd64.\n",
      "Preparing to unpack .../14-libcc1-0_10.3.0-1ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking libcc1-0:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Selecting previously unselected package libitm1:amd64.\n",
      "Preparing to unpack .../15-libitm1_10.3.0-1ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking libitm1:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Selecting previously unselected package libatomic1:amd64.\n",
      "Preparing to unpack .../16-libatomic1_10.3.0-1ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking libatomic1:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Selecting previously unselected package libasan5:amd64.\n",
      "Preparing to unpack .../17-libasan5_9.4.0-1ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking libasan5:amd64 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package liblsan0:amd64.\n",
      "Preparing to unpack .../18-liblsan0_10.3.0-1ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking liblsan0:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Selecting previously unselected package libtsan0:amd64.\n",
      "Preparing to unpack .../19-libtsan0_10.3.0-1ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking libtsan0:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Selecting previously unselected package libubsan1:amd64.\n",
      "Preparing to unpack .../20-libubsan1_10.3.0-1ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking libubsan1:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Selecting previously unselected package libquadmath0:amd64.\n",
      "Preparing to unpack .../21-libquadmath0_10.3.0-1ubuntu1~20.04_amd64.deb ...\n",
      "Unpacking libquadmath0:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Selecting previously unselected package libgcc-9-dev:amd64.\n",
      "Preparing to unpack .../22-libgcc-9-dev_9.4.0-1ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking libgcc-9-dev:amd64 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package gcc-9.\n",
      "Preparing to unpack .../23-gcc-9_9.4.0-1ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking gcc-9 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package gcc.\n",
      "Preparing to unpack .../24-gcc_4%3a9.3.0-1ubuntu2_amd64.deb ...\n",
      "Unpacking gcc (4:9.3.0-1ubuntu2) ...\n",
      "Selecting previously unselected package libstdc++-9-dev:amd64.\n",
      "Preparing to unpack .../25-libstdc++-9-dev_9.4.0-1ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking libstdc++-9-dev:amd64 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package g++-9.\n",
      "Preparing to unpack .../26-g++-9_9.4.0-1ubuntu1~20.04.1_amd64.deb ...\n",
      "Unpacking g++-9 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Selecting previously unselected package g++.\n",
      "Preparing to unpack .../27-g++_4%3a9.3.0-1ubuntu2_amd64.deb ...\n",
      "Unpacking g++ (4:9.3.0-1ubuntu2) ...\n",
      "Selecting previously unselected package make.\n",
      "Preparing to unpack .../28-make_4.2.1-1.2_amd64.deb ...\n",
      "Unpacking make (4.2.1-1.2) ...\n",
      "Selecting previously unselected package libdpkg-perl.\n",
      "Preparing to unpack .../29-libdpkg-perl_1.19.7ubuntu3.2_all.deb ...\n",
      "Unpacking libdpkg-perl (1.19.7ubuntu3.2) ...\n",
      "Selecting previously unselected package patch.\n",
      "Preparing to unpack .../30-patch_2.7.6-6_amd64.deb ...\n",
      "Unpacking patch (2.7.6-6) ...\n",
      "Selecting previously unselected package dpkg-dev.\n",
      "Preparing to unpack .../31-dpkg-dev_1.19.7ubuntu3.2_all.deb ...\n",
      "Unpacking dpkg-dev (1.19.7ubuntu3.2) ...\n",
      "Selecting previously unselected package build-essential.\n",
      "Preparing to unpack .../32-build-essential_12.8ubuntu1.1_amd64.deb ...\n",
      "Unpacking build-essential (12.8ubuntu1.1) ...\n",
      "Selecting previously unselected package libksba8:amd64.\n",
      "Preparing to unpack .../33-libksba8_1.3.5-2ubuntu0.20.04.2_amd64.deb ...\n",
      "Unpacking libksba8:amd64 (1.3.5-2ubuntu0.20.04.2) ...\n",
      "Selecting previously unselected package libnpth0:amd64.\n",
      "Preparing to unpack .../34-libnpth0_1.6-1_amd64.deb ...\n",
      "Unpacking libnpth0:amd64 (1.6-1) ...\n",
      "Selecting previously unselected package dirmngr.\n",
      "Preparing to unpack .../35-dirmngr_2.2.19-3ubuntu2.2_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.19-3ubuntu2.2) ...\n",
      "Selecting previously unselected package libfakeroot:amd64.\n",
      "Preparing to unpack .../36-libfakeroot_1.24-1_amd64.deb ...\n",
      "Unpacking libfakeroot:amd64 (1.24-1) ...\n",
      "Selecting previously unselected package fakeroot.\n",
      "Preparing to unpack .../37-fakeroot_1.24-1_amd64.deb ...\n",
      "Unpacking fakeroot (1.24-1) ...\n",
      "Selecting previously unselected package gnupg-l10n.\n",
      "Preparing to unpack .../38-gnupg-l10n_2.2.19-3ubuntu2.2_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.19-3ubuntu2.2) ...\n",
      "Selecting previously unselected package gnupg-utils.\n",
      "Preparing to unpack .../39-gnupg-utils_2.2.19-3ubuntu2.2_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.19-3ubuntu2.2) ...\n",
      "Selecting previously unselected package pinentry-curses.\n",
      "Preparing to unpack .../40-pinentry-curses_1.1.0-3build1_amd64.deb ...\n",
      "Unpacking pinentry-curses (1.1.0-3build1) ...\n",
      "Selecting previously unselected package gpg-agent.\n",
      "Preparing to unpack .../41-gpg-agent_2.2.19-3ubuntu2.2_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.19-3ubuntu2.2) ...\n",
      "Selecting previously unselected package gpg-wks-client.\n",
      "Preparing to unpack .../42-gpg-wks-client_2.2.19-3ubuntu2.2_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.19-3ubuntu2.2) ...\n",
      "Selecting previously unselected package gpg-wks-server.\n",
      "Preparing to unpack .../43-gpg-wks-server_2.2.19-3ubuntu2.2_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.19-3ubuntu2.2) ...\n",
      "Selecting previously unselected package gpgsm.\n",
      "Preparing to unpack .../44-gpgsm_2.2.19-3ubuntu2.2_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.19-3ubuntu2.2) ...\n",
      "Selecting previously unselected package gnupg.\n",
      "Preparing to unpack .../45-gnupg_2.2.19-3ubuntu2.2_all.deb ...\n",
      "Unpacking gnupg (2.2.19-3ubuntu2.2) ...\n",
      "Selecting previously unselected package libalgorithm-diff-perl.\n",
      "Preparing to unpack .../46-libalgorithm-diff-perl_1.19.03-2_all.deb ...\n",
      "Unpacking libalgorithm-diff-perl (1.19.03-2) ...\n",
      "Selecting previously unselected package libalgorithm-diff-xs-perl.\n",
      "Preparing to unpack .../47-libalgorithm-diff-xs-perl_0.04-6_amd64.deb ...\n",
      "Unpacking libalgorithm-diff-xs-perl (0.04-6) ...\n",
      "Selecting previously unselected package libalgorithm-merge-perl.\n",
      "Preparing to unpack .../48-libalgorithm-merge-perl_0.08-3_all.deb ...\n",
      "Unpacking libalgorithm-merge-perl (0.08-3) ...\n",
      "Selecting previously unselected package libfile-fcntllock-perl.\n",
      "Preparing to unpack .../49-libfile-fcntllock-perl_0.22-3build4_amd64.deb ...\n",
      "Unpacking libfile-fcntllock-perl (0.22-3build4) ...\n",
      "Setting up libksba8:amd64 (1.3.5-2ubuntu0.20.04.2) ...\n",
      "Setting up pinentry-curses (1.1.0-3build1) ...\n",
      "Setting up libfile-fcntllock-perl (0.22-3build4) ...\n",
      "Setting up libalgorithm-diff-perl (1.19.03-2) ...\n",
      "Setting up gpgsm (2.2.19-3ubuntu2.2) ...\n",
      "Setting up binutils-common:amd64 (2.34-6ubuntu1.4) ...\n",
      "Setting up libctf-nobfd0:amd64 (2.34-6ubuntu1.4) ...\n",
      "Setting up libnpth0:amd64 (1.6-1) ...\n",
      "Setting up libfakeroot:amd64 (1.24-1) ...\n",
      "Setting up fakeroot (1.24-1) ...\n",
      "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/fakeroot.1.gz because associated file /usr/share/man/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/faked.1.gz because associated file /usr/share/man/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/fakeroot.1.gz because associated file /usr/share/man/es/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/faked.1.gz because associated file /usr/share/man/es/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/fakeroot.1.gz because associated file /usr/share/man/fr/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/faked.1.gz because associated file /usr/share/man/fr/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/fakeroot.1.gz because associated file /usr/share/man/sv/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/faked.1.gz because associated file /usr/share/man/sv/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "Setting up dirmngr (2.2.19-3ubuntu2.2) ...\n",
      "Created symlink /etc/systemd/user/sockets.target.wants/dirmngr.socket → /usr/lib/systemd/user/dirmngr.socket.\n",
      "Setting up make (4.2.1-1.2) ...\n",
      "Setting up libmpfr6:amd64 (4.0.2-1) ...\n",
      "Setting up gnupg-l10n (2.2.19-3ubuntu2.2) ...\n",
      "Setting up xz-utils (5.2.4-1ubuntu1.1) ...\n",
      "update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist\n",
      "Setting up libquadmath0:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Setting up libmpc3:amd64 (1.1.0-1) ...\n",
      "Setting up libatomic1:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Setting up patch (2.7.6-6) ...\n",
      "Setting up libdpkg-perl (1.19.7ubuntu3.2) ...\n",
      "Setting up libubsan1:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Setting up libisl22:amd64 (0.22.1-1) ...\n",
      "Setting up libbinutils:amd64 (2.34-6ubuntu1.4) ...\n",
      "Setting up libalgorithm-diff-xs-perl (0.04-6) ...\n",
      "Setting up libcc1-0:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Setting up liblocale-gettext-perl (1.07-4) ...\n",
      "Setting up liblsan0:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Setting up libitm1:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Setting up gcc-9-base:amd64 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Setting up libalgorithm-merge-perl (0.08-3) ...\n",
      "Setting up gnupg-utils (2.2.19-3ubuntu2.2) ...\n",
      "Setting up libtsan0:amd64 (10.3.0-1ubuntu1~20.04) ...\n",
      "Setting up libctf0:amd64 (2.34-6ubuntu1.4) ...\n",
      "Setting up gpg-agent (2.2.19-3ubuntu2.2) ...\n",
      "Created symlink /etc/systemd/user/sockets.target.wants/gpg-agent-browser.socket → /usr/lib/systemd/user/gpg-agent-browser.socket.\n",
      "Created symlink /etc/systemd/user/sockets.target.wants/gpg-agent-extra.socket → /usr/lib/systemd/user/gpg-agent-extra.socket.\n",
      "Created symlink /etc/systemd/user/sockets.target.wants/gpg-agent-ssh.socket → /usr/lib/systemd/user/gpg-agent-ssh.socket.\n",
      "Created symlink /etc/systemd/user/sockets.target.wants/gpg-agent.socket → /usr/lib/systemd/user/gpg-agent.socket.\n",
      "Setting up gpg-wks-client (2.2.19-3ubuntu2.2) ...\n",
      "Setting up libasan5:amd64 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Setting up gpg-wks-server (2.2.19-3ubuntu2.2) ...\n",
      "Setting up gnupg (2.2.19-3ubuntu2.2) ...\n",
      "Setting up cpp-9 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Setting up binutils-x86-64-linux-gnu (2.34-6ubuntu1.4) ...\n",
      "Setting up binutils (2.34-6ubuntu1.4) ...\n",
      "Setting up dpkg-dev (1.19.7ubuntu3.2) ...\n",
      "Setting up libgcc-9-dev:amd64 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Setting up cpp (4:9.3.0-1ubuntu2) ...\n",
      "Setting up gcc-9 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Setting up libstdc++-9-dev:amd64 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Setting up gcc (4:9.3.0-1ubuntu2) ...\n",
      "Setting up g++-9 (9.4.0-1ubuntu1~20.04.1) ...\n",
      "Setting up g++ (4:9.3.0-1ubuntu2) ...\n",
      "update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/c++.1.gz because associated file /usr/share/man/man1/g++.1.gz (of link group c++) doesn't exist\n",
      "Setting up build-essential (12.8ubuntu1.1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.23.5)\n",
      "Collecting matplotlib>=2.1.0\n",
      "  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.4.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (23.0)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp310-cp310-linux_x86_64.whl size=102196 sha256=8d36c1256ddcb6528a2caa2fa7f8a56d6673a0732cabd139b6e7a93cc1246485\n",
      "  Stored in directory: /root/.cache/pip/wheels/58/e6/f9/f87c8f8be098b51b616871315318329cae12cdb618f4caac93\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, pycocotools\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.39.4 kiwisolver-1.4.4 matplotlib-3.7.1 pycocotools-2.0.6 pyparsing-3.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!apt-get install build-essential -y\n",
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Pwo0B44S2mZy",
   "metadata": {
    "id": "Pwo0B44S2mZy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "from retinanet import model as retinanet\n",
    "from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, \\\n",
    "    Normalizer\n",
    "\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as tdata\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import config\n",
    "import utils\n",
    "def get_transform(target_size, central_fraction=1.0):\n",
    "    return transforms.Compose([\n",
    "    transforms.Resize(int(target_size / central_fraction)),\n",
    "    transforms.CenterCrop(target_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def get_loader(train=False, val=False, test=False):\n",
    "    \"\"\" Returns a data loader for the desired split \"\"\"\n",
    "    assert train + val + test == 1, 'need to set exactly one of {train, val, test} to True'\n",
    "    split = VQA(\n",
    "        path_for(train=train, val=val, test=test, question=True),\n",
    "        path_for(train=train, val=val, test=test, answer=True),\n",
    "        config.preprocessed_path,\n",
    "        answerable_only=train,\n",
    "    )\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        split,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=train,  # only shuffle the data in training\n",
    "        pin_memory=True,\n",
    "        num_workers=config.data_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # put question lengths in descending order so that we can use packed sequences later\n",
    "    batch.sort(key=lambda x: x[-1], reverse=True)\n",
    "    return tdata.dataloader.default_collate(batch)\n",
    "\n",
    "\n",
    "class VQA(tdata.Dataset):\n",
    "    \"\"\" VQA dataset, open-ended \"\"\"\n",
    "    def __init__(self, questions_path, answers_path, image_features_path, answerable_only=False):\n",
    "        super(VQA, self).__init__()\n",
    "        with open(questions_path, 'r') as fd:\n",
    "            questions_json = json.load(fd)\n",
    "        with open(answers_path, 'r') as fd:\n",
    "            answers_json = json.load(fd)\n",
    "        with open(config.vocabulary_path, 'r') as fd:\n",
    "            vocab_json = json.load(fd)\n",
    "        self._check_integrity(questions_json, answers_json)\n",
    "\n",
    "        # vocab\n",
    "        self.vocab = vocab_json\n",
    "        self.token_to_index = self.vocab['question']\n",
    "        self.answer_to_index = self.vocab['answer']\n",
    "\n",
    "        # q and a\n",
    "        self.questions = list(prepare_questions(questions_json))\n",
    "        self.answers = list(prepare_answers(answers_json))\n",
    "        self.questions = [self._encode_question(q) for q in self.questions]\n",
    "        self.answers = [self._encode_answers(a) for a in self.answers]\n",
    "\n",
    "        # v\n",
    "        self.image_features_path = image_features_path\n",
    "        self.coco_id_to_index = self._create_coco_id_to_index()\n",
    "        self.coco_ids = [q['image_id'] for q in questions_json['questions']]\n",
    "\n",
    "        # only use questions that have at least one answer?\n",
    "        self.answerable_only = answerable_only\n",
    "        if self.answerable_only:\n",
    "            self.answerable = self._find_answerable()\n",
    "\n",
    "    @property\n",
    "    def max_question_length(self):\n",
    "        if not hasattr(self, '_max_length'):\n",
    "            self._max_length = max(map(len, self.questions))\n",
    "        return self._max_length\n",
    "\n",
    "    @property\n",
    "    def num_tokens(self):\n",
    "        return len(self.token_to_index) + 1  # add 1 for <unknown> token at index 0\n",
    "\n",
    "    def _create_coco_id_to_index(self):\n",
    "        \"\"\" Create a mapping from a COCO image id into the corresponding index into the h5 file \"\"\"\n",
    "        with h5py.File(self.image_features_path, 'r') as features_file:\n",
    "            coco_ids = features_file['ids'][()]\n",
    "        coco_id_to_index = {id: i for i, id in enumerate(coco_ids)}\n",
    "        return coco_id_to_index\n",
    "\n",
    "    def _check_integrity(self, questions, answers):\n",
    "        \"\"\" Verify that we are using the correct data \"\"\"\n",
    "        qa_pairs = list(zip(questions['questions'], answers['annotations']))\n",
    "        assert all(q['question_id'] == a['question_id'] for q, a in qa_pairs), 'Questions not aligned with answers'\n",
    "        assert all(q['image_id'] == a['image_id'] for q, a in qa_pairs), 'Image id of question and answer don\\'t match'\n",
    "        assert questions['data_type'] == answers['data_type'], 'Mismatched data types'\n",
    "        assert questions['data_subtype'] == answers['data_subtype'], 'Mismatched data subtypes'\n",
    "\n",
    "    def _find_answerable(self):\n",
    "        \"\"\" Create a list of indices into questions that will have at least one answer that is in the vocab \"\"\"\n",
    "        answerable = []\n",
    "        for i, answers in enumerate(self.answers):\n",
    "            answer_has_index = len(answers.nonzero()) > 0\n",
    "            # store the indices of anything that is answerable\n",
    "            if answer_has_index:\n",
    "                answerable.append(i)\n",
    "        return answerable\n",
    "\n",
    "    def _encode_question(self, question):\n",
    "        \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
    "        vec = torch.zeros(self.max_question_length).long()\n",
    "        for i, token in enumerate(question):\n",
    "            index = self.token_to_index.get(token, 0)\n",
    "            vec[i] = index\n",
    "        return vec, len(question)\n",
    "\n",
    "    def _encode_answers(self, answers):\n",
    "        \"\"\" Turn an answer into a vector \"\"\"\n",
    "        # answer vec will be a vector of answer counts to determine which answers will contribute to the loss.\n",
    "        # this should be multiplied with 0.1 * negative log-likelihoods that a model produces and then summed up\n",
    "        # to get the loss that is weighted by how many humans gave that answer\n",
    "        answer_vec = torch.zeros(len(self.answer_to_index))\n",
    "        for answer in answers:\n",
    "            index = self.answer_to_index.get(answer)\n",
    "            if index is not None:\n",
    "                answer_vec[index] += 1\n",
    "        return answer_vec\n",
    "\n",
    "    def _load_image(self, image_id):\n",
    "        \"\"\" Load an image \"\"\"\n",
    "        if not hasattr(self, 'features_file'):\n",
    "            # Loading the h5 file has to be done here and not in __init__ because when the DataLoader\n",
    "            # forks for multiple works, every child would use the same file object and fail\n",
    "            # Having multiple readers using different file objects is fine though, so we just init in here.\n",
    "            self.features_file = h5py.File(self.image_features_path, 'r')\n",
    "        index = self.coco_id_to_index[image_id]\n",
    "        dataset = self.features_file['features']\n",
    "        img = dataset[index].astype('float32')\n",
    "        return torch.from_numpy(img)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.answerable_only:\n",
    "            # change of indices to only address answerable questions\n",
    "            item = self.answerable[item]\n",
    "\n",
    "        q, q_length = self.questions[item]\n",
    "        a = self.answers[item]\n",
    "        image_id = self.coco_ids[item]\n",
    "        v = self._load_image(image_id)\n",
    "        # since batches are re-ordered for PackedSequence's, the original question order is lost\n",
    "        # we return `item` so that the order of (v, q, a) triples can be restored if desired\n",
    "        # without shuffling in the dataloader, these will be in the order that they appear in the q and a json's.\n",
    "        return v, q, a, item, q_length\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.answerable_only:\n",
    "            return len(self.answerable)\n",
    "        else:\n",
    "            return len(self.questions)\n",
    "\n",
    "\n",
    "# this is used for normalizing questions\n",
    "_special_chars = re.compile('[^a-z0-9 ]*')\n",
    "\n",
    "# these try to emulate the original normalization scheme for answers\n",
    "_period_strip = re.compile(r'(?!<=\\d)(\\.)(?!\\d)')\n",
    "_comma_strip = re.compile(r'(\\d)(,)(\\d)')\n",
    "_punctuation_chars = re.escape(r';/[]\"{}()=+\\_-><@`,?!')\n",
    "_punctuation = re.compile(r'([{}])'.format(re.escape(_punctuation_chars)))\n",
    "_punctuation_with_a_space = re.compile(r'(?<= )([{0}])|([{0}])(?= )'.format(_punctuation_chars))\n",
    "\n",
    "\n",
    "def prepare_questions(questions_json):\n",
    "    \"\"\" Tokenize and normalize questions from a given question json in the usual VQA format. \"\"\"\n",
    "    questions = [q['question'] for q in questions_json['questions']]\n",
    "    for question in questions:\n",
    "        question = question.lower()[:-1]\n",
    "        yield question.split(' ')\n",
    "\n",
    "\n",
    "def prepare_answers(answers_json):\n",
    "    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n",
    "    answers = [[a['answer'] for a in ans_dict['answers']] for ans_dict in answers_json['annotations']]\n",
    "    # The only normalization that is applied to both machine generated answers as well as\n",
    "    # ground truth answers is replacing most punctuation with space (see [0] and [1]).\n",
    "    # Since potential machine generated answers are just taken from most common answers, applying the other\n",
    "    # normalizations is not needed, assuming that the human answers are already normalized.\n",
    "    # [0]: http://visualqa.org/evaluation.html\n",
    "    # [1]: https://github.com/VT-vision-lab/VQA/blob/3849b1eae04a0ffd83f56ad6f70ebd0767e09e0f/PythonEvaluationTools/vqaEvaluation/vqaEval.py#L96\n",
    "\n",
    "    def process_punctuation(s):\n",
    "        # the original is somewhat broken, so things that look odd here might just be to mimic that behaviour\n",
    "        # this version should be faster since we use re instead of repeated operations on str's\n",
    "        if _punctuation.search(s) is None:\n",
    "            return s\n",
    "        s = _punctuation_with_a_space.sub('', s)\n",
    "        if re.search(_comma_strip, s) is not None:\n",
    "            s = s.replace(',', '')\n",
    "        s = _punctuation.sub(' ', s)\n",
    "        s = _period_strip.sub('', s)\n",
    "        return s.strip()\n",
    "\n",
    "    for answer_list in answers:\n",
    "        yield list(map(process_punctuation, answer_list))\n",
    "\n",
    "\n",
    "class CocoImages(tdata.Dataset):\n",
    "    \"\"\" Dataset for MSCOCO images located in a folder on the filesystem \"\"\"\n",
    "    def __init__(self, path, transform=None):\n",
    "        super(CocoImages, self).__init__()\n",
    "        self.path = path\n",
    "        self.id_to_filename = self._find_images()\n",
    "        self.sorted_ids = sorted(self.id_to_filename.keys())  # used for deterministic iteration order\n",
    "        print('found {} images in {}'.format(len(self), self.path))\n",
    "        self.transform = transform\n",
    "\n",
    "    def _find_images(self):\n",
    "        id_to_filename = {}\n",
    "        for filename in os.listdir(self.path):\n",
    "            if not filename.endswith('.jpg'):\n",
    "                continue\n",
    "            id_and_extension = filename.split('_')[-1]\n",
    "            id = int(id_and_extension.split('.')[0])\n",
    "            id_to_filename[id] = filename\n",
    "        return id_to_filename\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        id = self.sorted_ids[item]\n",
    "        path = os.path.join(self.path, self.id_to_filename[id])\n",
    "        img = Image.open(path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return id, img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sorted_ids)\n",
    "\n",
    "\n",
    "class Composite(tdata.Dataset):\n",
    "    \"\"\" Dataset that is a composite of several Dataset objects. Useful for combining splits of a dataset. \"\"\"\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        current = self.datasets[0]\n",
    "        for d in self.datasets:\n",
    "            if item < len(d):\n",
    "                return d[item]\n",
    "            item -= len(d)\n",
    "        else:\n",
    "            raise IndexError('Index too large for composite dataset')\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(map(len, self.datasets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b3bbc5-0285-48d9-a75d-5e7b67bbed3f",
   "metadata": {
    "id": "45b3bbc5-0285-48d9-a75d-5e7b67bbed3f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pathimg = \"/Visual-question-answering-model/train2014/\"\n",
    "pathimgval = \"/Visual-question-answering-model/val2014\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b49037c-13e4-4ced-86db-fb1d88f37083",
   "metadata": {
    "id": "8b49037c-13e4-4ced-86db-fb1d88f37083",
    "outputId": "4dafb699-32fa-48bc-a001-147eebc3bb0e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 82783 images in /Visual-question-answering-model/train2014/\n"
     ]
    }
   ],
   "source": [
    "train_data = CocoImages(pathimg)\n",
    "#    loader = create_coco_loader(pathimg,pathimgval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53fcda1-2de3-49f4-a320-5652fe7af0a4",
   "metadata": {
    "id": "e53fcda1-2de3-49f4-a320-5652fe7af0a4",
    "outputId": "e7697af2-1f21-4c06-a243-2b50a7f0e0c4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 82783 images in /Visual-question-answering-model/train2014/\n",
      "found 40504 images in /Visual-question-answering-model/val2014\n"
     ]
    }
   ],
   "source": [
    "pathes = [pathimg,pathimgval]\n",
    "transform = get_transform(config.image_size, config.central_fraction)\n",
    "datasets = [CocoImages(path, transform=transform) for path in pathes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08df3840-12d5-44e3-95c6-7e9110071b44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pretrainedmodels\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels) (0.15.2)\n",
      "Collecting munch\n",
      "  Downloading munch-3.0.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels) (4.65.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from munch->pretrainedmodels) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels) (3.1.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->pretrainedmodels) (1.23.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->pretrainedmodels) (2.29.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->pretrainedmodels) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->pretrainedmodels) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->pretrainedmodels) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->pretrainedmodels) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->pretrainedmodels) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->pretrainedmodels) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->pretrainedmodels) (1.3.0)\n",
      "Building wheels for collected packages: pretrainedmodels\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=a5d9f7fa2b7c745aa7625ff621b20b46cb13a819b7f208f65598a6baf654f996\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
      "Successfully built pretrainedmodels\n",
      "Installing collected packages: munch, pretrainedmodels\n",
      "Successfully installed munch-3.0.0 pretrainedmodels-0.7.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12736ab7-30b7-41da-9982-1db6e692e222",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu111/torch_stable.html\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.0+cu111 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.0+cu111\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.10.0+cu111 torchvision==0.11.1+cu111 -f https://download.pytorch.org/whl/cu111/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6WhxrqcokFCp",
   "metadata": {
    "id": "6WhxrqcokFCp",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (model): DenseNet(\n",
       "    (features): Sequential(\n",
       "      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu0): ReLU(inplace=True)\n",
       "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (denseblock1): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition1): _Transition(\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock2): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition2): _Transition(\n",
       "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock3): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer17): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer18): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer19): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer20): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer21): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer22): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer23): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer24): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer25): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer26): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer27): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer28): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer29): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer30): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer31): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer32): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer33): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer34): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1312, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer35): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1344, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer36): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1376, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer37): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1408, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer38): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1440, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer39): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1472, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer40): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1504, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer41): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer42): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1568, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer43): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1600, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer44): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer45): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1664, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer46): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1696, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1696, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer47): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1728, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer48): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1760, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1760, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition3): _Transition(\n",
       "        (norm): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(1792, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock4): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1312, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1344, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1376, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer17): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1408, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer18): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1440, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer19): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1472, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer20): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1504, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1504, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer21): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer22): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1568, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer23): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1600, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer24): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer25): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1664, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer26): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1696, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1696, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer27): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1728, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer28): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1760, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1760, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer29): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1792, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer30): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1824, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer31): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1856, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer32): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(1888, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1888, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm5): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (classifier): Linear(in_features=1920, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = models.densenet201(pretrained=True)  # Load Densenet-201 model\n",
    "\n",
    "        def save_output(module, input, output):\n",
    "            self.buffer = output\n",
    "\n",
    "        self.model.features.register_forward_hook(save_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.model(x)\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_coco_loader(*paths):\n",
    "    transform = utils.get_transform(config.image_size, config.central_fraction)\n",
    "    datasets = [data.CocoImages(path, transform=transform) for path in paths]\n",
    "    dataset = data.Composite(*datasets)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.preprocess_batch_size,\n",
    "        num_workers=config.data_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "cudnn.benchmark = True\n",
    "from data import CocoImages\n",
    "net = Net().cuda()\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09c8281f-87d2-4c73-8506-7f8b27e429ad",
   "metadata": {
    "id": "09c8281f-87d2-4c73-8506-7f8b27e429ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Composite(*datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f05dbb7-22f9-477d-b332-8f8751e3df3e",
   "metadata": {
    "id": "0f05dbb7-22f9-477d-b332-8f8751e3df3e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.preprocess_batch_size,\n",
    "        num_workers=config.data_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c45998c-ae75-4148-a21b-e91ef3d290b7",
   "metadata": {
    "id": "4c45998c-ae75-4148-a21b-e91ef3d290b7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "tH7yme3EMMVT",
   "metadata": {
    "id": "tH7yme3EMMVT",
    "outputId": "a85d719e-bc76-40cb-f572-ce0eec0cb3ae",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1927 [00:00<?, ?it/s]/tmp/ipykernel_7593/4283885064.py:15: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  imgs = Variable(imgs.cuda(), volatile=True)\n",
      "100%|██████████| 1927/1927 [20:46<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "    features_shape = (\n",
    "        len(loader.dataset),\n",
    "        1920,\n",
    "        config.output_size,\n",
    "        config.output_size\n",
    "    )\n",
    "\n",
    "    with h5py.File(config.preprocessed_path, 'w',libver='latest') as fd:\n",
    "        features = fd.create_dataset('features', shape=features_shape, dtype='float16')\n",
    "        coco_ids = fd.create_dataset('ids', shape=(len(loader.dataset),), dtype='int32')\n",
    "\n",
    "        i = j = 0\n",
    "        for ids, imgs in tqdm(loader):\n",
    "            with torch.no_grad():\n",
    "                imgs = Variable(imgs.cuda(), volatile=True)\n",
    "                out = net(imgs)\n",
    "\n",
    "                j = i + imgs.size(0)\n",
    "                features[i:j, :, :] = out.data.cpu().numpy().astype('float16')\n",
    "                coco_ids[i:j] = ids.numpy().astype('int32')\n",
    "                i = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38535874",
   "metadata": {
    "id": "38535874",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "571bf54a",
   "metadata": {
    "id": "571bf54a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre-process vocabularies for questions and answers from VQA 2.0 Abstract Scene: https://visualqa.org/vqa_v1_download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "KlH1qztYsq0g",
   "metadata": {
    "id": "KlH1qztYsq0g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_vocab(iterable, top_k=None, start=0):\n",
    "    \"\"\" Turns an iterable of list of tokens into a vocabulary.\n",
    "        These tokens could be single answers or word tokens in questions.\n",
    "    \"\"\"\n",
    "    all_tokens = itertools.chain.from_iterable(iterable)\n",
    "    counter = Counter(all_tokens)\n",
    "    if top_k:\n",
    "        most_common = counter.most_common(top_k)\n",
    "        most_common = (t for t, c in most_common)\n",
    "    else:\n",
    "        most_common = counter.keys()\n",
    "    # descending in count, then lexicographical order\n",
    "    tokens = sorted(most_common, key=lambda x: (counter[x], x), reverse=True)\n",
    "    vocab = {t: i for i, t in enumerate(tokens, start=start)}\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb8d8880-bc1e-42e5-81bf-0964d2a428d2",
   "metadata": {
    "id": "eb8d8880-bc1e-42e5-81bf-0964d2a428d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def path_for(train=False, val=False, test=False, question=False,answer =False):\n",
    "    assert train + val + test == 1\n",
    "    assert question + answer == 1\n",
    "    assert not (test and answer), 'loading answers from test split not supported'  # if you want to eval on test, you need to implement loading of a VQA Dataset without given answers yourself\n",
    "    if val and question:\n",
    "        fmt = 'v2_OpenEnded_mscoco_val2014_questions.json'\n",
    "    elif val and answer:\n",
    "        fmt = 'v2_mscoco_val2014_annotations.json'\n",
    "    elif answer and train:\n",
    "        fmt = 'v2_mscoco_train2014_annotations.json'\n",
    "    elif question and train:\n",
    "        fmt = 'v2_OpenEnded_mscoco_train2014_questions.json'\n",
    "    else:\n",
    "        fmt = '{1}_{2}_annotations.json'\n",
    "    s = fmt\n",
    "    qa_path = '/Visual-question-answering-model'  # directory containing the question and annotation jsons\n",
    "    return os.path.join(qa_path, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9Jg17j2lsq_K",
   "metadata": {
    "id": "9Jg17j2lsq_K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "    questions = path_for(train=True, question=True)\n",
    "    answers = path_for(train=True, answer=True)\n",
    "    with open(questions, 'r') as fd:\n",
    "        questions = json.load(fd)\n",
    "    with open(answers, 'r') as fd:\n",
    "        answers = json.load(fd)\n",
    "\n",
    "    questions = prepare_questions(questions)\n",
    "    answers = prepare_answers(answers)\n",
    "\n",
    "    question_vocab = extract_vocab(questions, start=1)\n",
    "    answer_vocab = extract_vocab(answers, top_k=config.max_answers)\n",
    "\n",
    "    vocabs = {\n",
    "        'question': question_vocab,\n",
    "        'answer': answer_vocab,\n",
    "    }\n",
    "    with open(config.vocabulary_path, 'w') as fd:\n",
    "        json.dump(vocabs, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G3K6c4pFsrB2",
   "metadata": {
    "id": "G3K6c4pFsrB2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UymeElGGsrEk",
   "metadata": {
    "id": "UymeElGGsrEk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bb335",
   "metadata": {
    "id": "836bb335"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91b8e2b3",
   "metadata": {
    "id": "91b8e2b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "uc7W9tW-s7X4",
   "metadata": {
    "id": "uc7W9tW-s7X4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_learning_rate(optimizer, iteration):\n",
    "    lr = config.initial_lr * 0.5**(float(iteration) / config.lr_halflife)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "total_iterations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0d0d6df-cea8-4fb3-9b6c-cba8d933d130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import config\n",
    "\n",
    "\n",
    "class model8(nn.Module):\n",
    "    \"\"\" Re-implementation of ``Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering'' [0]\n",
    "\n",
    "    [0]: https://arxiv.org/abs/1704.03162\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_tokens):\n",
    "        super(model8, self).__init__()\n",
    "        question_features = 1024\n",
    "        vision_features =1920\n",
    "        glimpses = 2\n",
    "\n",
    "        self.text = TextProcessor(\n",
    "            embedding_tokens=embedding_tokens,\n",
    "            embedding_features=300,\n",
    "            lstm_features=question_features,\n",
    "            drop=0.5,\n",
    "        )\n",
    "        self.attention = Attention(\n",
    "            v_features=vision_features,\n",
    "            q_features=question_features,\n",
    "            mid_features=512,\n",
    "            glimpses=2,\n",
    "            drop=0.5,\n",
    "        )\n",
    "        self.classifier = Classifier(\n",
    "            in_features=glimpses * vision_features + question_features,\n",
    "            mid_features=1024,\n",
    "            out_features=config.max_answers,\n",
    "            drop=0.5,\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "                init.xavier_uniform(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, v, q, q_len):\n",
    "        q = self.text(q, list(q_len.data))\n",
    "\n",
    "        v = v / (v.norm(p=2, dim=1, keepdim=True).expand_as(v) + 1e-8)\n",
    "        a = self.attention(v, q)\n",
    "        v = apply_attention(v, a)\n",
    "\n",
    "        combined = torch.cat([v, q], dim=1)\n",
    "        answer = self.classifier(combined)\n",
    "        return answer, a\n",
    "\n",
    "\n",
    "class Classifier(nn.Sequential):\n",
    "    def __init__(self, in_features, mid_features, out_features, drop=0.0):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.add_module('drop1', nn.Dropout(drop))\n",
    "        self.add_module('lin1', nn.Linear(in_features, mid_features))\n",
    "        self.add_module('relu', nn.ReLU())\n",
    "        self.add_module('drop2', nn.Dropout(drop))\n",
    "        self.add_module('lin2', nn.Linear(mid_features, out_features))\n",
    "\n",
    "\n",
    "def load_skipgram_embeddings():\n",
    "    # Load the pretrained Skip-gram embeddings from a file or any other source\n",
    "    # For this example, let's create a random embeddings matrix\n",
    "    embedding_tokens = train_loader.dataset.num_tokens  # Number of tokens\n",
    "    embedding_features = 300  # Embedding dimensions\n",
    "    pretrained_embeddings = np.random.randn(embedding_tokens, embedding_features)\n",
    "    return pretrained_embeddings\n",
    "\n",
    "class TextProcessor(nn.Module):\n",
    "    def __init__(self, embedding_tokens, embedding_features, lstm_features, drop=0.0):\n",
    "        super(TextProcessor, self).__init__()\n",
    "        self.embedding = nn.Embedding(embedding_tokens, embedding_features, padding_idx=0)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(input_size=embedding_features,\n",
    "                            hidden_size=lstm_features,\n",
    "                            num_layers=1)\n",
    "        self.features = lstm_features\n",
    "\n",
    "        self._init_lstm(self.lstm.weight_ih_l0)\n",
    "        self._init_lstm(self.lstm.weight_hh_l0)\n",
    "        self.lstm.bias_ih_l0.data.zero_()\n",
    "        self.lstm.bias_hh_l0.data.zero_()\n",
    "\n",
    "        # Load pretrained Skip-gram embeddings\n",
    "        pretrained_embeddings = load_skipgram_embeddings()  # Replace with your own implementation\n",
    "        embedding_tokens, embedding_features = pretrained_embeddings.shape\n",
    "\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_embeddings).float())\n",
    "        self.embedding.weight.requires_grad = False  # Freeze the embeddings during training if desired\n",
    "\n",
    "        init.xavier_uniform_(self.lstm.weight_ih_l0)\n",
    "        init.xavier_uniform_(self.lstm.weight_hh_l0)\n",
    "\n",
    "    def _init_lstm(self, weight):\n",
    "        for w in weight.chunk(4, 0):\n",
    "            init.xavier_uniform_(w)\n",
    "\n",
    "    def forward(self, q, q_len):\n",
    "        embedded = self.embedding(q)\n",
    "        tanhed = self.tanh(self.drop(embedded))\n",
    "        packed = pack_padded_sequence(tanhed, q_len, batch_first=True)\n",
    "        _, (_, c) = self.lstm(packed)\n",
    "        return c.squeeze(0)\n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, v_features, q_features, mid_features, glimpses, drop=0.0):\n",
    "        super(Attention, self).__init__()\n",
    "        self.v_conv = nn.Conv2d(v_features, mid_features, 1, bias=False)  # let self.lin take care of bias\n",
    "        self.q_lin = nn.Linear(q_features, mid_features)\n",
    "        self.x_conv = nn.Conv2d(mid_features, glimpses, 1)\n",
    "\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, v, q):\n",
    "        v = self.v_conv(self.drop(v))\n",
    "        q = self.q_lin(self.drop(q))\n",
    "        q = tile_2d_over_nd(q, v)\n",
    "        x = self.relu(v + q)\n",
    "        x = self.x_conv(self.drop(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def apply_attention(input, attention):\n",
    "    \"\"\" Apply any number of attention maps over the input. \"\"\"\n",
    "    n, c = input.size()[:2]\n",
    "    glimpses = attention.size(1)\n",
    "\n",
    "    # flatten the spatial dims into the third dim, since we don't need to care about how they are arranged\n",
    "    input = input.view(n, 1, c, -1) # [n, 1, c, s]\n",
    "    attention = attention.view(n, glimpses, -1)\n",
    "    attention = F.softmax(attention, dim=-1).unsqueeze(2) # [n, g, 1, s]\n",
    "    weighted = attention * input # [n, g, v, s]\n",
    "    weighted_mean = weighted.sum(dim=-1) # [n, g, v]\n",
    "    return weighted_mean.view(n, -1)\n",
    "\n",
    "\n",
    "def tile_2d_over_nd(feature_vector, feature_map):\n",
    "    \"\"\" Repeat the same feature vector over all spatial positions of a given feature map.\n",
    "        The feature vector should have the same batch size and number of features as the feature map.\n",
    "    \"\"\"\n",
    "    n, c = feature_vector.size()\n",
    "    spatial_size = feature_map.dim() - 2\n",
    "    tiled = feature_vector.view(n, c, *([1] * spatial_size)).expand_as(feature_map)\n",
    "    return tiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "IHmrUH3GtDuA",
   "metadata": {
    "id": "IHmrUH3GtDuA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(net, loader, optimizer, tracker, train=False, prefix='', epoch=0):\n",
    "    \"\"\" Run an epoch over the given loader \"\"\"\n",
    "    if train:\n",
    "        net.train()\n",
    "        tracker_class, tracker_params = tracker.MovingMeanMonitor, {'momentum': 0.99}\n",
    "    else:\n",
    "        net.eval()\n",
    "        tracker_class, tracker_params = tracker.MeanMonitor, {}\n",
    "        answ = []\n",
    "        idxs = []\n",
    "        accs = []\n",
    "\n",
    "    tq = tqdm(loader, desc='{} E{:03d}'.format(prefix, epoch), ncols=0)\n",
    "    loss_tracker = tracker.track('{}_loss'.format(prefix), tracker_class(**tracker_params))\n",
    "    acc_tracker = tracker.track('{}_acc'.format(prefix), tracker_class(**tracker_params))\n",
    "\n",
    "    log_softmax = nn.LogSoftmax().cuda()\n",
    "    for v, q, a, idx, q_len in tq:\n",
    "        var_params = {\n",
    "            'volatile': not train,\n",
    "            'requires_grad': False,\n",
    "        }\n",
    "        v = Variable(v.cuda(), **var_params)\n",
    "        q = Variable(q.cuda(), **var_params)\n",
    "        a = Variable(a.cuda(), **var_params)\n",
    "        q_len = Variable(q_len.cuda(), **var_params)\n",
    "\n",
    "        out,_ = net(v, q, q_len)\n",
    "        nll = -log_softmax(out)\n",
    "        loss = (nll * a / 10).sum(dim=1).mean()\n",
    "        acc = utils.batch_accuracy(out.data, a.data).cpu()\n",
    "\n",
    "        if train:\n",
    "            global total_iterations\n",
    "            update_learning_rate(optimizer, total_iterations)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_iterations += 1\n",
    "        else:\n",
    "            # store information about evaluation of this minibatch\n",
    "            _, answer = out.data.cpu().max(dim=1)\n",
    "            answ.append(answer.view(-1))\n",
    "            accs.append(acc.view(-1))\n",
    "            idxs.append(idx.view(-1).clone())\n",
    "\n",
    "        loss_tracker.append(loss.item())\n",
    "        # acc_tracker.append(acc.mean())\n",
    "        for a in acc:\n",
    "            acc_tracker.append(a.item())\n",
    "        fmt = '{:.4f}'.format\n",
    "        tq.set_postfix(loss=fmt(loss_tracker.mean.value), acc=fmt(acc_tracker.mean.value))\n",
    "\n",
    "    if not train:\n",
    "        answ = list(torch.cat(answ, dim=0))\n",
    "        accs = list(torch.cat(accs, dim=0))\n",
    "        idxs = list(torch.cat(idxs, dim=0))\n",
    "        return answ, accs, idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mjkcRqpKtEZq",
   "metadata": {
    "id": "mjkcRqpKtEZq",
    "outputId": "27f14849-1fba-4f41-f8d0-de0c7a55e7df",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will save to logs/model8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7593/4017860180.py:42: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(m.weight)\n",
      "train E000:   0% 0/3396 [00:00<?, ?it/s]/tmp/ipykernel_7593/1024819654.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nll = -log_softmax(out)\n",
      "train E000: 100% 3396/3396 [06:33<00:00,  8.63it/s, acc=0.4481, loss=2.1724]\n",
      "val E000:   0% 0/1675 [00:00<?, ?it/s]/tmp/ipykernel_7593/1024819654.py:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  v = Variable(v.cuda(), **var_params)\n",
      "/tmp/ipykernel_7593/1024819654.py:24: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  q = Variable(q.cuda(), **var_params)\n",
      "/tmp/ipykernel_7593/1024819654.py:25: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  a = Variable(a.cuda(), **var_params)\n",
      "/tmp/ipykernel_7593/1024819654.py:26: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  q_len = Variable(q_len.cuda(), **var_params)\n",
      "val E000: 100% 1675/1675 [03:11<00:00,  8.74it/s, acc=0.4328, loss=2.0059]\n",
      "train E001: 100% 3396/3396 [06:17<00:00,  9.00it/s, acc=0.4436, loss=2.0078]\n",
      "val E001: 100% 1675/1675 [03:59<00:00,  7.00it/s, acc=0.4641, loss=1.8591]\n",
      "train E002: 100% 3396/3396 [06:04<00:00,  9.31it/s, acc=0.4819, loss=1.9148]\n",
      "val E002: 100% 1675/1675 [03:17<00:00,  8.47it/s, acc=0.4742, loss=1.7920]\n",
      "train E003: 100% 3396/3396 [07:43<00:00,  7.33it/s, acc=0.5345, loss=1.8695]\n",
      "val E003:  54% 897/1675 [02:12<02:21,  5.48it/s, acc=0.4900, loss=1.7681]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E004:  80% 2722/3396 [05:02<01:16,  8.82it/s, acc=0.4760, loss=1.8637]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E004: 100% 1675/1675 [03:15<00:00,  8.58it/s, acc=0.4982, loss=1.7367]\n",
      "train E005:  34% 1149/3396 [02:28<05:10,  7.24it/s, acc=0.4911, loss=1.8222]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E005: 100% 3396/3396 [07:25<00:00,  7.63it/s, acc=0.5910, loss=1.8164]\n",
      "val E005:  63% 1063/1675 [02:31<01:23,  7.31it/s, acc=0.5031, loss=1.7163]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E006:  67% 2259/3396 [04:13<02:01,  9.36it/s, acc=0.4979, loss=1.7951]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E006: 100% 1675/1675 [03:12<00:00,  8.70it/s, acc=0.5086, loss=1.7088]\n",
      "train E007:  27% 901/3396 [01:46<04:57,  8.39it/s, acc=0.5187, loss=1.7512]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E007: 100% 3396/3396 [06:24<00:00,  8.83it/s, acc=0.5147, loss=1.7779]\n",
      "val E007:  68% 1136/1675 [02:43<01:15,  7.16it/s, acc=0.5113, loss=1.6997]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E008:  79% 2671/3396 [06:04<01:35,  7.57it/s, acc=0.5622, loss=1.7462]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E008: 100% 1675/1675 [03:26<00:00,  8.09it/s, acc=0.5177, loss=1.6894]\n",
      "train E009:  33% 1112/3396 [02:03<03:53,  9.76it/s, acc=0.5052, loss=1.7292]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E009: 100% 3396/3396 [06:11<00:00,  9.13it/s, acc=0.5312, loss=1.7230]\n",
      "val E009:  84% 1409/1675 [02:42<00:30,  8.85it/s, acc=0.5219, loss=1.6869]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E010:  65% 2202/3396 [05:01<02:46,  7.19it/s, acc=0.5855, loss=1.7397]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E010:  88% 3005/3396 [06:46<00:50,  7.79it/s, acc=0.5656, loss=1.7229]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E010: 100% 1675/1675 [03:55<00:00,  7.12it/s, acc=0.5244, loss=1.6802]\n",
      "train E011:  14% 492/3396 [01:05<06:53,  7.03it/s, acc=0.5545, loss=1.6887]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E011:  36% 1238/3396 [02:43<04:01,  8.94it/s, acc=0.6559, loss=1.7010]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E011: 100% 3396/3396 [07:31<00:00,  7.53it/s, acc=0.4825, loss=1.7144]\n",
      "val E011:  25% 416/1675 [01:02<02:24,  8.69it/s, acc=0.5276, loss=1.6707]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E011:  67% 1117/1675 [02:33<01:02,  8.87it/s, acc=0.5258, loss=1.6693]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E012:  62% 2114/3396 [03:58<02:21,  9.07it/s, acc=0.5134, loss=1.7144]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E012:  85% 2893/3396 [05:22<00:52,  9.53it/s, acc=0.5837, loss=1.6985]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E012: 100% 1675/1675 [03:58<00:00,  7.04it/s, acc=0.5302, loss=1.6620]\n",
      "train E013:  13% 453/3396 [01:06<06:49,  7.20it/s, acc=0.5611, loss=1.6898]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E013:  32% 1093/3396 [02:33<05:07,  7.49it/s, acc=0.5956, loss=1.6756]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E013: 100% 3396/3396 [07:39<00:00,  7.40it/s, acc=0.5509, loss=1.6803]\n",
      "val E013:  18% 295/1675 [00:47<03:19,  6.92it/s, acc=0.5279, loss=1.6675]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E013:  62% 1040/1675 [02:31<01:27,  7.28it/s, acc=0.5295, loss=1.6640]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E014:  59% 2020/3396 [03:44<02:27,  9.33it/s, acc=0.5094, loss=1.6695]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E014:  84% 2847/3396 [05:14<00:57,  9.49it/s, acc=0.5328, loss=1.6734]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E014: 100% 1675/1675 [03:10<00:00,  8.78it/s, acc=0.5324, loss=1.6609]\n",
      "train E015:  26% 893/3396 [01:47<04:23,  9.48it/s, acc=0.5590, loss=1.6609]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E015: 100% 3396/3396 [06:22<00:00,  8.88it/s, acc=0.5450, loss=1.6632]\n",
      "val E015:  64% 1073/1675 [02:34<01:25,  7.00it/s, acc=0.5331, loss=1.6625]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E016:  84% 2864/3396 [05:22<01:08,  7.77it/s, acc=0.5744, loss=1.6710]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E016: 100% 1675/1675 [03:08<00:00,  8.88it/s, acc=0.5375, loss=1.6529]\n",
      "train E017:  40% 1348/3396 [02:32<03:29,  9.77it/s, acc=0.5812, loss=1.6397]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E017: 100% 3396/3396 [06:09<00:00,  9.20it/s, acc=0.5459, loss=1.6543]\n",
      "val E017:  95% 1583/1675 [03:18<00:10,  8.74it/s, acc=0.5376, loss=1.6497]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E018: 100% 3396/3396 [06:12<00:00,  9.13it/s, acc=0.5104, loss=1.6370]\n",
      "val E018:   6% 104/1675 [00:17<02:51,  9.16it/s, acc=0.5367, loss=1.6888]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E019:  52% 1760/3396 [03:56<03:31,  7.75it/s, acc=0.5212, loss=1.6278]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E019: 100% 1668/1675 [03:53<00:00,  7.89it/s, acc=0.5388, loss=1.6476]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E020: 100% 3396/3396 [07:02<00:00,  8.03it/s, acc=0.5732, loss=1.6451]\n",
      "val E020:   5% 78/1675 [00:15<03:01,  8.79it/s, acc=0.5398, loss=1.6909] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E021:  59% 1994/3396 [03:47<02:31,  9.27it/s, acc=0.5999, loss=1.6196]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E021: 100% 3396/3396 [06:18<00:00,  8.97it/s, acc=0.5699, loss=1.6106]\n",
      "val E021:  23% 383/1675 [00:59<02:56,  7.31it/s, acc=0.5430, loss=1.6482]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E021: 100% 1675/1675 [03:59<00:00,  7.00it/s, acc=0.5423, loss=1.6404]\n",
      "train E022:  18% 606/3396 [01:09<04:56,  9.40it/s, acc=0.5134, loss=1.6011]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E022:  67% 2285/3396 [04:08<01:58,  9.38it/s, acc=0.5541, loss=1.6184]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E022: 100% 3396/3396 [06:09<00:00,  9.20it/s, acc=0.5627, loss=1.6177]\n",
      "val E022:  49% 813/1675 [01:39<01:40,  8.61it/s, acc=0.5442, loss=1.6376]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E023:  19% 660/3396 [01:29<05:36,  8.14it/s, acc=0.5317, loss=1.5953]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E023:  70% 2367/3396 [05:02<02:36,  6.58it/s, acc=0.5832, loss=1.6157]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E023:  28% 473/1675 [01:17<02:56,  6.82it/s, acc=0.5453, loss=1.6377]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E023: 100% 1675/1675 [04:07<00:00,  6.78it/s, acc=0.5433, loss=1.6393]\n",
      "train E024:  15% 519/3396 [01:16<07:10,  6.68it/s, acc=0.5178, loss=1.6007]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E024:  64% 2159/3396 [05:07<02:52,  7.17it/s, acc=0.5480, loss=1.6020]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E024: 100% 3396/3396 [07:55<00:00,  7.15it/s, acc=0.5267, loss=1.5963]\n",
      "val E024:  25% 422/1675 [01:05<03:04,  6.77it/s, acc=0.5459, loss=1.6393]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E025:  13% 439/3396 [01:10<07:16,  6.78it/s, acc=0.6314, loss=1.5905]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E025:  61% 2075/3396 [04:50<02:48,  7.86it/s, acc=0.5346, loss=1.5884]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E025:  26% 431/1675 [00:55<02:20,  8.83it/s, acc=0.5459, loss=1.6360]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E025: 100% 1675/1675 [03:14<00:00,  8.60it/s, acc=0.5451, loss=1.6365]\n",
      "train E026:  14% 486/3396 [01:08<07:15,  6.69it/s, acc=0.5877, loss=1.5814]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E026:  65% 2211/3396 [05:09<02:49,  6.98it/s, acc=0.5648, loss=1.6021]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E026: 100% 3396/3396 [07:53<00:00,  7.18it/s, acc=0.6183, loss=1.5984]\n",
      "val E026:  22% 373/1675 [00:52<03:03,  7.08it/s, acc=0.5475, loss=1.6394]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E027:  53% 1815/3396 [03:26<03:16,  8.05it/s, acc=0.5926, loss=1.6018]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E027: 100% 1675/1675 [04:01<00:00,  6.94it/s, acc=0.5481, loss=1.6343]\n",
      "train E028:  10% 333/3396 [00:52<07:01,  7.27it/s, acc=0.5864, loss=1.5881]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E028: 100% 3396/3396 [07:52<00:00,  7.18it/s, acc=0.5687, loss=1.5938]\n",
      "val E028:  19% 310/1675 [00:51<03:20,  6.81it/s, acc=0.5455, loss=1.6367]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E028: 100% 1675/1675 [04:08<00:00,  6.74it/s, acc=0.5478, loss=1.6324]\n",
      "train E029:  18% 622/3396 [01:29<07:21,  6.29it/s, acc=0.5824, loss=1.5837]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E029:  57% 1939/3396 [04:34<03:15,  7.46it/s, acc=0.6124, loss=1.5833]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E029: 100% 3396/3396 [07:55<00:00,  7.14it/s, acc=0.5794, loss=1.5850]\n",
      "val E029:  30% 507/1675 [01:18<02:49,  6.91it/s, acc=0.5501, loss=1.6267]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E029: 100% 1675/1675 [04:11<00:00,  6.65it/s, acc=0.5489, loss=1.6286]\n",
      "train E030:   6% 212/3396 [00:38<08:19,  6.37it/s, acc=0.5411, loss=1.5597]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E030:  63% 2133/3396 [05:25<02:57,  7.13it/s, acc=0.6184, loss=1.5671]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E030: 100% 3396/3396 [08:33<00:00,  6.62it/s, acc=0.5961, loss=1.5764]\n",
      "val E030:   8% 132/1675 [00:26<03:54,  6.58it/s, acc=0.5466, loss=1.6630]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E030: 100% 1675/1675 [04:23<00:00,  6.37it/s, acc=0.5493, loss=1.6281]\n",
      "train E031:  12% 403/3396 [00:51<05:39,  8.81it/s, acc=0.6462, loss=1.5597]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E031:  56% 1889/3396 [03:35<02:36,  9.64it/s, acc=0.5642, loss=1.5621]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E031: 100% 3396/3396 [06:22<00:00,  8.88it/s, acc=0.5959, loss=1.5633]\n",
      "val E031:  31% 520/1675 [01:15<02:51,  6.75it/s, acc=0.5515, loss=1.6290]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E031: 100% 1675/1675 [04:02<00:00,  6.89it/s, acc=0.5493, loss=1.6320]\n",
      "train E032:   7% 243/3396 [00:35<06:44,  7.80it/s, acc=0.5204, loss=1.5664]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E032:  63% 2131/3396 [04:54<02:35,  8.14it/s, acc=0.5629, loss=1.5687]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E032: 100% 3396/3396 [07:42<00:00,  7.34it/s, acc=0.5738, loss=1.5609]\n",
      "val E032:   9% 154/1675 [00:24<02:46,  9.14it/s, acc=0.5456, loss=1.6561]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E032: 100% 1675/1675 [04:01<00:00,  6.93it/s, acc=0.5503, loss=1.6311]\n",
      "train E033:  10% 341/3396 [00:55<07:42,  6.61it/s, acc=0.5853, loss=1.5757]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train E033: 100% 3396/3396 [07:50<00:00,  7.22it/s, acc=0.5538, loss=1.5734]\n",
      "val E033: 100% 1675/1675 [04:13<00:00,  6.62it/s, acc=0.5507, loss=1.6318]\n",
      "train E034: 100% 3396/3396 [07:42<00:00,  7.34it/s, acc=0.5753, loss=1.5642]\n",
      "train E037:  92% 3135/3396 [06:18<00:28,  9.15it/s, acc=0.6013, loss=1.5367]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "val E037: 100% 1675/1675 [04:08<00:00,  6.73it/s, acc=0.5524, loss=1.6261]\n",
      "train E038: 100% 3396/3396 [08:06<00:00,  6.99it/s, acc=0.5237, loss=1.5555]\n",
      "val E038: 100% 1675/1675 [04:01<00:00,  6.95it/s, acc=0.5526, loss=1.6241]\n",
      "train E039: 100% 3396/3396 [08:10<00:00,  6.92it/s, acc=0.5706, loss=1.5454]\n",
      "val E039: 100% 1675/1675 [03:14<00:00,  8.60it/s, acc=0.5532, loss=1.6230]\n",
      "train E040: 100% 3396/3396 [07:36<00:00,  7.45it/s, acc=0.6158, loss=1.5323]\n",
      "train E041: 100% 3396/3396 [06:11<00:00,  9.13it/s, acc=0.6248, loss=1.5474]\n",
      "val E041: 100% 1675/1675 [03:21<00:00,  8.31it/s, acc=0.5541, loss=1.6226]\n",
      "train E042: 100% 3396/3396 [07:57<00:00,  7.12it/s, acc=0.5911, loss=1.5345]\n",
      "val E042: 100% 1675/1675 [04:12<00:00,  6.64it/s, acc=0.5545, loss=1.6232]\n",
      "train E043:  10% 324/3396 [00:50<05:37,  9.10it/s, acc=0.5488, loss=1.5338]"
     ]
    }
   ],
   "source": [
    "    import torch.optim as optim\n",
    "    from torch.autograd import Variable\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    from tqdm import tqdm\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    name = datetime.now().strftime(\"model8\")\n",
    "    target_name = os.path.join('logs', '{}.pth'.format(name))\n",
    "    print('will save to {}'.format(target_name))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    train_loader = get_loader(train=True)\n",
    "    val_loader = get_loader(val=True)\n",
    "\n",
    "    net = model8(train_loader.dataset.num_tokens).cuda()\n",
    "    optimizer = optim.Adam([p for p in net.parameters() if p.requires_grad])\n",
    "\n",
    "    tracker = utils.Tracker()\n",
    "    config_as_dict = {k: v for k, v in vars(config).items() if not k.startswith('__')}\n",
    "\n",
    "    for i in range(config.epochs):\n",
    "        _ = run(net, train_loader, optimizer, tracker, train=True, prefix='train', epoch=i)\n",
    "        r = run(net, val_loader, optimizer, tracker, train=False, prefix='val', epoch=i)\n",
    "\n",
    "        results = {\n",
    "        'name': name,\n",
    "        'tracker': tracker.to_dict(),\n",
    "        'config': config_as_dict,\n",
    "        'weights': net.state_dict(),\n",
    "        'eval': {\n",
    "            'answers': r[0],\n",
    "            'accuracies': r[1],\n",
    "            'idx': r[2],\n",
    "        },\n",
    "        'vocab': train_loader.dataset.vocab,\n",
    "    }\n",
    "        # Create the parent directory if it doesn't exist\n",
    "        os.makedirs('logs', exist_ok=True)\n",
    "        torch.save(results, target_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6df82",
   "metadata": {
    "id": "02b6df82",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot diagram based on training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZZkeUiw3tjyu",
   "metadata": {
    "id": "ZZkeUiw3tjyu",
    "outputId": "e2e0e6a8-7c64-4129-e01b-470f5b10d43c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path =\"/Visual-question-answering-model/logs/2023-05-07_12:25:00.pth\"\n",
    "results = torch.load(path)\n",
    "val_acc = torch.FloatTensor(results['tracker']['val_acc'])\n",
    "val_acc = val_acc.mean(dim=1).numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(val_acc)\n",
    "plt.savefig('val_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b33a7f",
   "metadata": {
    "id": "84b33a7f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_questions(questions):\n",
    "    '''\n",
    "    Remove punctuation marks and spaces. Returns list\n",
    "    '''\n",
    "    questions = [questions]\n",
    "    for question in questions:\n",
    "        question = question.lower()[:-1]\n",
    "        yield question.split(' ')\n",
    "\n",
    "def encode_question(question):\n",
    "    '''\n",
    "    Encode questions\n",
    "    Get ids using vocabulary created using tokens during training\n",
    "    '''\n",
    "    vec = torch.zeros(len(question)).long()\n",
    "    with open(config.vocabulary_path, 'r') as fd:\n",
    "        vocab_json = json.load(fd)\n",
    "    token_to_index = vocab_json['question']\n",
    "    for i, token in enumerate(question):\n",
    "        index = token_to_index.get(token, 0)\n",
    "        vec[i] = index\n",
    "    return vec, len(question)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "    Loading Resnet pretrained model to get image features\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = caffe_resnet.resnet152(pretrained=True)\n",
    "\n",
    "        def save_output(module, input, output):\n",
    "            self.buffer = output\n",
    "        self.model.layer4.register_forward_hook(save_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.model(x)\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "def encode_img(net,img_path):\n",
    "    '''\n",
    "    Encoding input image using Resnet features. Resizes input image to config.image_size\n",
    "    '''\n",
    "    cudnn.benchmark = True\n",
    "    transform = get_transform(config.image_size, config.central_fraction)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = transform(img)\n",
    "    ix,iy = img.size()[1],img.size()[2]\n",
    "    net = Net().cuda()\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        img = Variable(img.cuda())\n",
    "        out = net(img.view(1,3,ix,iy))\n",
    "        features = out.data.cpu().numpy().astype('float32')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc32078-4bbf-4b25-93c5-9fcb2654a780",
   "metadata": {
    "id": "0cc32078-4bbf-4b25-93c5-9fcb2654a780",
    "outputId": "b15a61ed-68cb-4412-affe-f421339e6f9f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485dccb",
   "metadata": {
    "id": "2485dccb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import torch\n",
    "import model\n",
    "\n",
    "def demo(img_path,question):\n",
    "    '''\n",
    "    Main demo function. Takes input image path and Question string as input\n",
    "    Returns top 5 answers, shows input image and visualizes attention applied \n",
    "    '''\n",
    "    print('The Question asked was: ',question)\n",
    "    cudnn.benchmark = True\n",
    "    # Load pre-trained image\n",
    "    log = torch.load('/Visual-question-answering-model/logs/2023-05-07_12:25:00.pth')\n",
    "    tokens = len(log['vocab']['question']) + 1\n",
    "    net = model8(tokens).cpu()\n",
    "    net.load_state_dict(log['weights'])\n",
    "    net.eval()\n",
    "    \n",
    "    questions = list(prepare_questions(question))\n",
    "    questions = [encode_question(q) for q in questions]\n",
    "    q,q_len = questions[0]\n",
    "    q = q.unsqueeze(0)\n",
    "\n",
    "    v = encode_img(net,img_path)\n",
    "    v = torch.from_numpy(v).to(torch.float)\n",
    "    q_len = torch.tensor([q_len])\n",
    "    with torch.no_grad():\n",
    "        v = Variable(v)\n",
    "        q = Variable(q)\n",
    "        q_len = Variable(q_len)  \n",
    "    \n",
    "    out,att_out = net.forward(v,q,q_len)\n",
    "    out = out.data.cpu()\n",
    "    _, answer5 = torch.topk(out,5)\n",
    "    answers = []\n",
    "    with open(config.vocabulary_path, 'r') as fd:\n",
    "        vocab_json = json.load(fd)\n",
    "    a_to_i = vocab_json['answer']\n",
    "    for answer in answer5:\n",
    "        answer = (answer.view(-1))\n",
    "        for a in answer.data:\n",
    "            answers.append(list(a_to_i.keys())[a.data])        \n",
    "    print_answers(answers)\n",
    "    visualize_attentn(att_out,img_path)\n",
    "    return\n",
    "\n",
    "def visualize_attentn(att_out,img_path):\n",
    "    '''\n",
    "    Takes output of attention layer and overlays on input image. Then shows both \n",
    "    '''\n",
    "    att_out = att_out.view(-1,14,14)\n",
    "    num_im = att_out.size()[0]\n",
    "    im = Image.open(img_path)\n",
    "    fig2,ax2 = plt.subplots(1)\n",
    "    ax2.imshow(im)\n",
    "    ax2.set_title('Original Image')\n",
    "    ax2.axis('off')\n",
    "    fig,axs = plt.subplots(1,num_im,figsize=(10,10))\n",
    "    axs = axs.ravel()\n",
    "    for i in range(0,num_im):\n",
    "        a1 = att_out[i].cpu().detach()\n",
    "        a1 = a1.numpy()\n",
    "        a1 = skimage.transform.pyramid_expand(a1, upscale=64)\n",
    "        im = im.resize(a1.shape)\n",
    "        axs[i].imshow(im)\n",
    "        axs[i].imshow(a1,cmap='gray',alpha=0.65)\n",
    "        axs[i].set_title('Attention image '+str(i))\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "    return\n",
    "\n",
    "def print_answers(answers):\n",
    "    '''\n",
    "    Function to print top 5 answers\n",
    "    '''\n",
    "    for i,a in enumerate(answers):\n",
    "        print(\"The top \",i+1,\" answer is \",a)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bec047-7018-4d9f-95b1-414e25481fd7",
   "metadata": {
    "id": "a4bec047-7018-4d9f-95b1-414e25481fd7",
    "outputId": "ab9e3893-4f61-4a36-9d84-dc731efb97da",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo('download.jpeg','How many of them are wearing black pants?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e664a9c-246b-4ac2-8cf0-4fee9411a9b3",
   "metadata": {
    "id": "9e664a9c-246b-4ac2-8cf0-4fee9411a9b3",
    "outputId": "e4b9ef5c-a4f2-4d9e-97cd-87dda41b66b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo('images.jpeg','What color is the chair?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ba215-186a-4ac9-afd1-40dce505c447",
   "metadata": {
    "id": "5c7ba215-186a-4ac9-afd1-40dce505c447",
    "outputId": "5b424c63-6106-4224-8b4b-2a4474805b48",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo('download.jpeg','what they are doing?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4470460-c30d-4ad0-aefe-91bd3f2660c7",
   "metadata": {
    "id": "a4470460-c30d-4ad0-aefe-91bd3f2660c7",
    "outputId": "043d667d-73b8-47d8-da33-3497cc408510",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo('random-acts-of-kindness-article-1200x800.jpg','what are they carring?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26238f5-6248-4681-85d9-30e672d6ce02",
   "metadata": {
    "id": "f26238f5-6248-4681-85d9-30e672d6ce02",
    "outputId": "c31d8aa6-caa2-4f17-d0df-154e1cea1b65",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo('images.jpeg','where is the chair?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d59001-da31-427e-98db-a87b207b607e",
   "metadata": {
    "id": "21d59001-da31-427e-98db-a87b207b607e",
    "outputId": "f50ff834-a90e-4019-ba14-5ae1f994b6db",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo('photographer-rock-15840594.jpg','what does he do?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55044b8b-9daa-49e8-81e9-c988103d84e5",
   "metadata": {
    "id": "55044b8b-9daa-49e8-81e9-c988103d84e5",
    "outputId": "4a28540a-fc4b-4bf5-b80e-bb46e655dedf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo('photographer-rock-15840594.jpg','describe the weather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44819177-403b-4dde-bc5e-1b17cec89507",
   "metadata": {
    "id": "44819177-403b-4dde-bc5e-1b17cec89507",
    "outputId": "524aef8c-c0ce-4774-a215-f47ee3d6d771",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo('photographer-rock-15840594.jpg','describe the weather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310199cd-435e-4c34-8a52-097eb0257133",
   "metadata": {
    "id": "310199cd-435e-4c34-8a52-097eb0257133"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
